{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1920e30",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:/BaiduSyncdisk/data/Climate_Economy/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m wksp \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:/BaiduSyncdisk/data/Climate_Economy/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m os\u001b[38;5;241m.\u001b[39mchdir(wksp)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:/BaiduSyncdisk/data/Climate_Economy/'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "wksp = 'D:/BaiduSyncdisk/data/Climate_Economy/'\n",
    "os.chdir(wksp)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ipyleaflet import Map, Marker\n",
    "import geopandas as gpd\n",
    "import rasterio as rio\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import convolve2d\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c4e2a9",
   "metadata": {},
   "source": [
    "# MLP for integrating DMSP and VIIRS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2aac0a4",
   "metadata": {},
   "source": [
    "Before finalizing the model, we used grid search to adjust the hyperparameters. Although sklearn provides a hyperparameter optimization method, \n",
    "it cannot meet our purpose of comparing training time and generalization ability, so we wrote our own grid search code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54691e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import multiprocessing\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import scipy.stats as stats\n",
    "\n",
    "import os\n",
    "\n",
    "wksp = r'\\\\10.95.10.91\\d/BaiduSyncdisk/data/Climate_Economy/'\n",
    "os.chdir(wksp)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ipyleaflet import Map, Marker\n",
    "import geopandas as gpd\n",
    "import rasterio as rio\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import convolve2d\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from multiprocessing import Process, Manager\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_model(optimizer, learning_rate, activation, neurons1, neurons2, dropout_rate, input_shape):\n",
    "    model = Sequential([\n",
    "        Dense(neurons1, activation=activation, input_shape=(input_shape,)),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(neurons2, activation=activation),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    if optimizer == 'adam':\n",
    "        opt = Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999)\n",
    "    elif optimizer == 'sgd':\n",
    "        opt = SGD(learning_rate=learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        opt = RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=opt, loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_and_evaluate(params, ttv_datas_all, results_list, mod_i):\n",
    "    print(mod_i,'started')\n",
    "    optimizer, learning_rate, neurons1, neurons2, activation, dropout_rate = params\n",
    "    train_x_ht_all, train_y_ht_all, test_x_ht_all, test_y_ht_all, vali_x_ht_all, vali_y_ht_all = ttv_datas_all\n",
    "    model = create_model(optimizer, learning_rate, activation, neurons1, neurons2, dropout_rate, train_x_ht_all.shape[1])\n",
    "    start_time = time.time()\n",
    "    history = model.fit(train_x_ht_all,\n",
    "                        train_y_ht_all,\n",
    "                        epochs=1000,\n",
    "                        batch_size=10000,\n",
    "                        # validation_split=0.05,\n",
    "                        validation_data=(vali_x_ht_all, vali_y_ht_all),\n",
    "                        callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)],\n",
    "                        verbose=1\n",
    "                        )\n",
    "\n",
    "    training_time = time.time() - start_time\n",
    "    train_loss = history.history['loss'][-1]\n",
    "    val_loss = history.history['val_loss'][-1]\n",
    "    pred_y_test = model.predict(test_x_ht_all, batch_size=100000)\n",
    "    R_test = stats.pearsonr((np.array(test_y_ht_all)).reshape(-1, ), (pred_y_test).reshape(-1, ))[0]\n",
    "\n",
    "    pred_y_train = model.predict(train_x_ht_all, batch_size=100000)\n",
    "    R_train = stats.pearsonr((np.array(train_y_ht_all)).reshape(-1, ), (pred_y_train).reshape(-1, ))[0]\n",
    "\n",
    "    pred_y_vali = model.predict(vali_x_ht_all, batch_size=100000)\n",
    "    R_vali = stats.pearsonr((np.array(vali_y_ht_all)).reshape(-1, ), (pred_y_vali).reshape(-1, ))[0]\n",
    "\n",
    "    print(f\"Params: {params}, Loss: {train_loss:.4f},R:{R_test:.4f}, MAE: {val_loss:.4f}, Time: {training_time:.2f} sec\")\n",
    "    print(mod_i, 'finished')\n",
    "    results_list.append((params, R_test, R_train, R_vali, train_loss, val_loss, training_time))\n",
    "\n",
    "\n",
    "mod_i = 0\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    continentList = [\"Africa\", \"Asia\", \"Europe\", \"South America\", \"Oceania\", \"North America\"]  #\n",
    "    # 打开栅格文件\n",
    "    # continentList = [\"Africa\"]\n",
    "    train_x_ht_all = np.array([]).reshape(0, 27)\n",
    "    train_y_ht_all = np.array([]).reshape(0, 1)\n",
    "\n",
    "    test_x_ht_all = np.array([]).reshape(0, 27)\n",
    "    test_y_ht_all = np.array([]).reshape(0, 1)\n",
    "\n",
    "    vali_x_ht_all = np.array([]).reshape(0, 27)\n",
    "    vali_y_ht_all = np.array([]).reshape(0, 1)\n",
    "\n",
    "    for ste in (continentList):\n",
    "        print(ste)\n",
    "        data_name = 'Nei2' + ste + '2013'\n",
    "        # data_df = pd.read_csv('./Light_GDP/PSO_BPNN/ALLHiLoMedianLag3_odr2'+ste+MaxLimitLab+LogDataLab+'.csv',sep = ',')\n",
    "        data_df = pd.read_csv('./Light_GDP/PSO_BPNN/' + data_name + '.csv', sep=',')\n",
    "\n",
    "        data_df.head\n",
    "\n",
    "        # 设置随机数种子\n",
    "        tf.random.set_seed(42)\n",
    "\n",
    "        # 生成训练数据\n",
    "\n",
    "        # XV = ['X','Y','lagnpp','lagnpp2','DistSat', 'VNPP']\n",
    "        XV = ['X', 'Y'\n",
    "            , 'N11', 'N12', 'N13',\n",
    "              'N14', 'N0', 'N15',\n",
    "              'N16', 'N17', 'N18',\n",
    "              'N21', 'N22', 'N23', 'N24', 'N25', 'N26', 'N27', 'N28', 'N29', 'N210', 'N211', 'N212', 'N213', 'N214',\n",
    "              'N215',\n",
    "              'N216'\n",
    "              ]\n",
    "        YV = ['VDMSP']\n",
    "        #\n",
    "\n",
    "        data_len = data_df.shape[0]\n",
    "        # train_len = int(data_df.shape[0]*0.6)\n",
    "        scaler_x = MinMaxScaler()\n",
    "        scaler_y = MinMaxScaler()\n",
    "        # train_x = np.array(data_df.VNPP[0:train_len]).reshape(-1,1)\n",
    "        # train_y = np.array(data_df.VDMSP[0:train_len]).reshape(-1,1)\n",
    "        # train_x, test_x, train_y, test_y = train_test_split(data_df.loc[:,['X','Y','lagnpp','lagnpp2','DistSat','VNPP']], data_df.loc[:,['VDMSP']], test_size=0.4, random_state=42)\n",
    "\n",
    "        # train_x, test_x, train_y, test_y = train_test_split(data_df.loc[:,XV], data_df.loc[:,['VDMSP']], test_size=0.4, random_state=42)\n",
    "        # 分段采样\n",
    "        breaks_dic = {\n",
    "            'Africa': [-1, 7.057890251, 26.21502093, 53.4383119, 85.70295305, 120.9924043, 156.2818556, 188.5464967,\n",
    "                       220.8111378, 300],\n",
    "            'Asia': [-1, 6.049620215, 25.2067509, 53.4383119, 85.70295305, 120.9924043, 156.2818556, 189.5547667,\n",
    "                     220.8111378, 300],\n",
    "            'Europe': [-1, 8.066160287, 28.231561, 56.46312201, 89.73603319, 126.0337545, 160.3149357, 193.5878469,\n",
    "                       226.8607581, 300],\n",
    "            'North America': [-1, 7.057890251, 26.21502093, 54.44658194, 88.72776315, 126.0337545, 162.3314758,\n",
    "                              197.620927,\n",
    "                              231.9021082, 300],\n",
    "            'Oceania': [-1, 6.049620215, 25.2067509, 54.44658194, 89.73603319, 126.0337545, 162.3314758, 197.620927,\n",
    "                        230.8938382, 300],\n",
    "            'South America': [-1, 6.049620215, 25.2067509, 53.4383119, 85.70295305, 120.9924043, 157.2901256,\n",
    "                              192.5795768,\n",
    "                              227.8690281, 300]\n",
    "        }\n",
    "\n",
    "        breaks = breaks_dic[ste]\n",
    "        if 'Log' in data_name:\n",
    "            breaks = np.log(breaks)\n",
    "            breaks[0] = -100\n",
    "\n",
    "        vi_train = np.array([])\n",
    "        vi_test = np.array([])\n",
    "        vi_vali = np.array([])\n",
    "        len(breaks)\n",
    "        import numpy as np\n",
    "\n",
    "        total = 9\n",
    "        max_quantile = 0.5\n",
    "        quants = np.linspace(1, total, total)\n",
    "        quants = np.power(quants, 2)\n",
    "        quants = quants / (total ** 2) * max_quantile\n",
    "        print(quants)\n",
    "\n",
    "        for bki in range(1, len(breaks)):\n",
    "\n",
    "            vi = np.arange(0, data_df.shape[0])\n",
    "            vi_inbk = vi[np.logical_and(data_df[YV[0]] > breaks[bki - 1], data_df[YV[0]] <= breaks[bki])]\n",
    "            vi_inbk_train, vi_inbk_test = train_test_split(vi_inbk, test_size=0.2)\n",
    "\n",
    "\n",
    "            vi_inbk_train, vi_inbk_vali = train_test_split(vi_inbk_train, test_size=0.3)\n",
    "            vi_train = np.append(vi_train, vi_inbk_train)\n",
    "            vi_test = np.append(vi_test, vi_inbk_test)\n",
    "            vi_vali = np.append(vi_vali, vi_inbk_vali)\n",
    "\n",
    "        # train_x, test_x, train_y, test_y = train_test_split(data_df.loc[:,XV], data_df.loc[:,['VDMSP']], test_size=0.4, random_state=42)\n",
    "\n",
    "        train_x = data_df.loc[vi_train, XV]\n",
    "        train_y = data_df.loc[vi_train, YV]\n",
    "\n",
    "        test_x = data_df.loc[vi_test, XV]\n",
    "        test_y = data_df.loc[vi_test, YV]\n",
    "\n",
    "        vali_x = data_df.loc[vi_vali, XV]\n",
    "        vali_y = data_df.loc[vi_vali, YV]\n",
    "\n",
    "\n",
    "\n",
    "        if True:  # Is or Not log : False for better\n",
    "\n",
    "            XV_log = ['N11', 'N12', 'N13'\n",
    "                , 'N14', 'N0', 'N15', 'N16', 'N17', 'N18'\n",
    "                , 'N21', 'N22', 'N23', 'N24', 'N25', 'N26', 'N27', 'N28', 'N29', 'N210', 'N211', 'N212', 'N213', 'N214',\n",
    "                      'N215', 'N216'\n",
    "                      ]\n",
    "\n",
    "            # train_y.loc[:,YV] = np.log(train_y.loc[:,YV]+1)\n",
    "            train_x.loc[:, XV_log] = np.log(train_x.loc[:, XV_log] + 1)\n",
    "            # vali_y.loc[:,YV] = np.log(vali_y.loc[:,YV]+1)\n",
    "            vali_x.loc[:, XV_log] = np.log(vali_x.loc[:, XV_log] + 1)\n",
    "            # test_y.loc[:,YV] = np.log(test_y.loc[:,YV]+1)\n",
    "            test_x.loc[:, XV_log] = np.log(test_x.loc[:, XV_log] + 1)\n",
    "\n",
    "        if True:  # Is or Not Normalization : Is for better or worse?\n",
    "            scaler_x.fit_transform(data_df.loc[:, XV])\n",
    "            scaler_y.fit_transform(data_df.loc[:, YV])\n",
    "            train_x = scaler_x.transform(train_x)\n",
    "            train_y = scaler_y.transform(train_y)\n",
    "            test_x = scaler_x.transform(test_x)\n",
    "            test_y = scaler_y.transform(test_y)\n",
    "            vali_x = scaler_x.transform(vali_x)\n",
    "            vali_y = scaler_y.transform(vali_y)\n",
    "        t, train_x_ht, t1, train_y_ht = train_test_split(train_x, train_y, test_size=0.01, random_state=2)\n",
    "        t, test_x_ht, t1, test_y_ht = train_test_split(test_x, test_y, test_size=0.01, random_state=2)\n",
    "        t, vali_x_ht, t1, vali_y_ht = train_test_split(vali_x, vali_y, test_size=0.01, random_state=2)\n",
    "\n",
    "        train_x_ht_all = np.concatenate([train_x_ht_all, train_x_ht])\n",
    "        train_y_ht_all = np.concatenate([train_y_ht_all, train_y_ht])\n",
    "        test_x_ht_all = np.concatenate([test_x_ht_all, test_x_ht])\n",
    "        test_y_ht_all = np.concatenate([test_y_ht_all, test_y_ht])\n",
    "        vali_x_ht_all = np.concatenate([vali_x_ht_all, vali_x_ht])\n",
    "        vali_y_ht_all = np.concatenate([vali_y_ht_all, vali_y_ht])\n",
    "\n",
    "    # Set up workspace and other configurations\n",
    "    # Data loading and preprocessing\n",
    "    # Define param_grid\n",
    "    # Setup Manager and results_list\n",
    "    # 定义超参数范围\n",
    "    param_grid = {\n",
    "        'model__optimizer': ['adam', 'sgd', 'rmsprop'],#,\n",
    "        'model__learning_rate': [0.001, 0.01],#\n",
    "        'model__neurons1': [100, 200, 300],#\n",
    "        'model__neurons2': [100, 200, 300],#\n",
    "        'model__activation': ['relu', 'tanh'],#\n",
    "        'model__dropout_rate': [0.1, 0.2,0.3]#\n",
    "    }\n",
    "\n",
    "    #param_grid = {\n",
    "    #    'model__optimizer': ['adam'],#\n",
    "    #    'model__learning_rate': [0.001],#\n",
    "    #    'model__neurons1': [150],#\n",
    "    #    'model__neurons2': [200],#\n",
    "    #    'model__activation': ['tanh'],#, 'tanh'\n",
    "    #    'model__dropout_rate': [0.2]#\n",
    "    #}\n",
    "\n",
    "    # 生成参数组合\n",
    "    param_combinations = list(itertools.product(\n",
    "        param_grid['model__optimizer'],\n",
    "        param_grid['model__learning_rate'],\n",
    "        param_grid['model__neurons1'],\n",
    "        param_grid['model__neurons2'],\n",
    "        param_grid['model__activation'],\n",
    "        param_grid['model__dropout_rate']\n",
    "    ))\n",
    "    ttv_datas_all = [train_x_ht_all, train_y_ht_all,test_x_ht_all,test_y_ht_all,vali_x_ht_all,vali_y_ht_all]\n",
    "    # 使用多进程处理\n",
    "    manager = Manager()\n",
    "    results_list = manager.list()\n",
    "    processes_all = []\n",
    "    processes = []\n",
    "    # Start multiprocessing\n",
    "    for params in param_combinations:\n",
    "        mod_i+=1\n",
    "        print(mod_i,'initialization')\n",
    "        p = Process(target=train_and_evaluate, args=(params, ttv_datas_all, results_list,mod_i))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "        processes_all.append(p)\n",
    "        if len(processes) > 0:\n",
    "            for p in processes:\n",
    "                p.join()\n",
    "            processes = []\n",
    "\n",
    "    for p in processes_all:\n",
    "        p.join()\n",
    "\n",
    "    # 输出结果\n",
    "    for result in results_list:\n",
    "        print(result)\n",
    "    print((results_list))\n",
    "    results_list_df = pd.DataFrame(results_list[0:])\n",
    "    results_list_df.columns = ['params', 'R_test', 'R_train', 'R_vali', 'train_loss', 'val_loss', 'training_time']\n",
    "    results_list_df.to_csv(r'\\\\10.95.10.91\\d\\BaiduSyncdisk\\data\\Climate_Economy\\Light_GDP\\revised1_results/HT_results_list_01_MLP_RRR.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c5fff9",
   "metadata": {},
   "source": [
    "Training model: First, divide the training set, verification set and test set into sections. Although there are literatures in the process of data processing, as the value of DMSP light increases, the value of one-sided or two-sided quantiles is continuously increased, so that better accuracy can be obtained, but there is no way to prove that these values ​​are outliers from a statistical point of view. Then this method of discarding may be problematic, so the processing of quantile discarding is not adopted. Data standardization uses the same scale as the standardization of predicted data, and cannot be standardized based on input data. Here is the multi-layer perceptron deep learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250df438",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "wksp = 'D:/BaiduSyncdisk/data/Climate_Economy/'\n",
    "os.chdir(wksp)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ipyleaflet import Map, Marker\n",
    "import geopandas as gpd\n",
    "import rasterio as rio\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import convolve2d\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "RmSat = False\n",
    "MaxLimit = False\n",
    "LogData = False\n",
    "\n",
    "\n",
    "if MaxLimit:\n",
    "    MaxLimitLab = 'MaxFilt'\n",
    "else:\n",
    "    MaxLimitLab = ''\n",
    "\n",
    "if LogData:\n",
    "    LogDataLab = 'Log'\n",
    "else:\n",
    "    LogDataLab = ''\n",
    "\n",
    "continentList = [ \"Oceania\", \"North America\"] #  \"Africa\", \"Asia\",\"Europe\", \"South America\",\n",
    "\n",
    "continentList = [\"Africa\"]    \n",
    "for ste in continentList:\n",
    "    data_name = 'Nei2'+ste+'2013'\n",
    "    #data_df = pd.read_csv('./Light_GDP/PSO_BPNN/ALLHiLoMedianLag3_odr2'+ste+MaxLimitLab+LogDataLab+'.csv',sep = ',')\n",
    "    data_df = pd.read_csv('./Light_GDP/PSO_BPNN/'+data_name+'.csv',sep = ',')\n",
    "\n",
    "    data_df.head\n",
    "\n",
    "\n",
    "\n",
    "    # set seed\n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "    #XV = ['X','Y','lagnpp','lagnpp2','DistSat', 'VNPP']\n",
    "    XV = ['X','Y'\n",
    "          ,'N11','N12','N13',\n",
    "          'N14','N0','N15',\n",
    "          'N16','N17','N18',\n",
    "          'N21','N22','N23','N24','N25','N26','N27','N28','N29','N210','N211','N212','N213','N214','N215','N216'\n",
    "          ]\n",
    "    YV = ['VDMSP']\n",
    "    \n",
    "    ### Divide the data into training set, validation set and test set\n",
    "    data_len = data_df.shape[0]\n",
    "    #train_len = int(data_df.shape[0]*0.6)\n",
    "    scaler_x = MinMaxScaler()\n",
    "    scaler_y = MinMaxScaler()\n",
    "    \n",
    "    breaks_dic = {'Africa':[-1,7.057890251,26.21502093,53.4383119,85.70295305,120.9924043,156.2818556,188.5464967,220.8111378,300],\n",
    "              'Asia':[-1,6.049620215,25.2067509,53.4383119,85.70295305,120.9924043,156.2818556,189.5547667,220.8111378,300],\n",
    "              'Europe':[-1,8.066160287,28.231561,56.46312201,89.73603319,126.0337545,160.3149357,193.5878469,226.8607581,300],\n",
    "              'North America':[-1,7.057890251,26.21502093,54.44658194,88.72776315,126.0337545,162.3314758,197.620927,231.9021082,300],\n",
    "              'Oceania':[-1,6.049620215,25.2067509,54.44658194,89.73603319,126.0337545,162.3314758,197.620927,230.8938382,300],\n",
    "              'South America':[-1,6.049620215,25.2067509,53.4383119,85.70295305,120.9924043,157.2901256,192.5795768,227.8690281,300]\n",
    "             }\n",
    "\n",
    "    breaks = breaks_dic[ste]\n",
    "    if 'Log' in data_name:\n",
    "        breaks = np.log(breaks)\n",
    "        breaks[0] = -100\n",
    "\n",
    "    vi_train = np.array([])\n",
    "    vi_test = np.array([])\n",
    "    vi_vali = np.array([])\n",
    "    len(breaks)\n",
    "    import numpy as np\n",
    "    total = 9\n",
    "    max_quantile=0.5\n",
    "    quants = np.linspace(1, total, total)  \n",
    "    quants = np.power(quants, 2)\n",
    "    quants = quants / (total ** 2) * max_quantile\n",
    "    print(quants)\n",
    "    \n",
    "        \n",
    "    for bki in range(1,len(breaks)):\n",
    "        \n",
    "        vi = np.arange(0,data_df.shape[0])\n",
    "        vi_inbk = vi[np.logical_and(data_df[YV[0]]> breaks[bki-1], data_df[YV[0]] <= breaks[bki])]\n",
    "        vi_inbk_train, vi_inbk_test = train_test_split(vi_inbk, test_size=0.2)\n",
    "\n",
    "        if False: # Remove noise by distribution\n",
    "            VNPP_tmp = data_df.loc[vi_inbk_train,['VNPP']]\n",
    "            VDMSP_tmp = data_df.loc[vi_inbk_train,['VDMSP']]\n",
    "            VNPP_Z = (VNPP_tmp - np.mean(VNPP_tmp))/np.std(VNPP_tmp)\n",
    "            vi_inbk_train = vi_inbk_train[np.array(np.logical_and(VNPP_Z> -3,VNPP_Z < 3 )).reshape(-1,)]\n",
    "\n",
    "            VNPP_tmp_test = data_df.loc[vi_inbk_test,['VNPP']]\n",
    "            VDMSP_tmp_test = data_df.loc[vi_inbk_test,['VDMSP']]\n",
    "\n",
    "            VNPP_Z_test = (VNPP_tmp_test - np.mean(VNPP_tmp_test))/np.std(VNPP_tmp_test)\n",
    "            vi_inbk_test = vi_inbk_test[np.array(np.logical_and(VNPP_Z_test> -3,VNPP_Z_test < 3 )).reshape(-1,)]\n",
    "\n",
    "        if False:# Remove Noise by quantiale\n",
    "\n",
    "            VNPP_tmp = data_df.loc[vi_inbk_train,['VNPP']]\n",
    "            VDMSP_tmp = data_df.loc[vi_inbk_train,['VDMSP']]\n",
    "\n",
    "            vi_inbk_train = vi_inbk_train[np.array(np.logical_and(VNPP_tmp>= np.quantile(VNPP_tmp,[np.exp(-(10-bki)),0.95])[0],\n",
    "                                                                  1)).reshape(-1,)]\n",
    "            VNPP_tmp_test = data_df.loc[vi_inbk_test,['VNPP']]\n",
    "            VDMSP_tmp_test = data_df.loc[vi_inbk_test,['VDMSP']]\n",
    "            vi_inbk_test = vi_inbk_test[np.array(np.logical_and(VNPP_tmp_test>= np.quantile(VNPP_tmp_test,[np.exp(-(10-bki)),0.95])[0],\n",
    "                                                                  1 )).reshape(-1,)]\n",
    "\n",
    "        vi_inbk_train, vi_inbk_vali = train_test_split(vi_inbk_train, test_size=0.3)\n",
    "        vi_train = np.append(vi_train, vi_inbk_train)\n",
    "        vi_test = np.append(vi_test, vi_inbk_test)\n",
    "        vi_vali = np.append(vi_vali, vi_inbk_vali)\n",
    "\n",
    "    #train_x, test_x, train_y, test_y = train_test_split(data_df.loc[:,XV], data_df.loc[:,['VDMSP']], test_size=0.4, random_state=42)\n",
    "\n",
    "    train_x = data_df.loc[vi_train,XV]\n",
    "    train_y = data_df.loc[vi_train,YV]\n",
    "    \n",
    "    test_x = data_df.loc[vi_test,XV]\n",
    "    test_y = data_df.loc[vi_test,YV]\n",
    "\n",
    "    vali_x = data_df.loc[vi_vali,XV]\n",
    "    vali_y = data_df.loc[vi_vali,YV]\n",
    "    \n",
    "    \n",
    "    #np.save('./Light_GDP/PSO_BPNN/model_save/MLP_'+ste+'_vi_train.npy',vi_train)\n",
    "    #np.save('./Light_GDP/PSO_BPNN/model_save/MLP_'+ste+'_vi_vali.npy',vi_vali)\n",
    "    #np.save('./Light_GDP/PSO_BPNN/model_save/MLP_'+ste+'_vi_test.npy',vi_test)\n",
    "\n",
    "    \n",
    "    if True: # Is or Not log : False for better \n",
    "        \n",
    "        XV_log = ['N11','N12','N13'\n",
    "          ,'N14','N0','N15','N16','N17','N18'\n",
    "          ,'N21','N22','N23','N24','N25','N26','N27','N28','N29','N210','N211','N212','N213','N214','N215','N216'\n",
    "          ]\n",
    "        \n",
    "        #train_y.loc[:,YV] = np.log(train_y.loc[:,YV]+1)\n",
    "        train_x.loc[:,XV_log] = np.log(train_x.loc[:,XV_log]+1)\n",
    "        #vali_y.loc[:,YV] = np.log(vali_y.loc[:,YV]+1)\n",
    "        vali_x.loc[:,XV_log] = np.log(vali_x.loc[:,XV_log]+1)\n",
    "        #test_y.loc[:,YV] = np.log(test_y.loc[:,YV]+1)\n",
    "        test_x.loc[:,XV_log] = np.log(test_x.loc[:,XV_log]+1)\n",
    "\n",
    "    if True: # Is or Not Normalization : Is for better or worse?\n",
    "        scaler_x.fit_transform(data_df.loc[:,XV])\n",
    "        scaler_y.fit_transform(data_df.loc[ :,YV])\n",
    "        train_x = scaler_x.transform(train_x)\n",
    "        train_y = scaler_y.transform(train_y)\n",
    "        test_x = scaler_x.transform(test_x)\n",
    "        test_y = scaler_y.transform(test_y)\n",
    "        vali_x = scaler_x.transform(vali_x)\n",
    "        vali_y = scaler_y.transform(vali_y)\n",
    "\n",
    "\n",
    "    # define a DNN model\n",
    "    def Create_model():\n",
    "        model = tf.keras.Sequential([\n",
    "            #tf.keras.layers.BatchNormalization((train_x.shape[1],)),\n",
    "            tf.keras.layers.Dense(200, activation='relu', # After testing different activation function, relu get a better accuracy\n",
    "                                  input_shape=(train_x.shape[1],)\n",
    "                                 ),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "\n",
    "            tf.keras.layers.Dense(200, activation='relu', # After testing different activation function, relu get a better accuracy\n",
    "                                  input_shape=(train_x.shape[1],)\n",
    "                                 ),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "\n",
    "\n",
    "\n",
    "            tf.keras.layers.Dense(1)\n",
    "        ])\n",
    "\n",
    "        # compile the model\n",
    "        Adam = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
    "        sgd = tf.keras.optimizers.SGD(learning_rate = 0.01)\n",
    "        RMSProp = tf.keras.optimizers.RMSprop(learning_rate = 0.01)\n",
    "        model.compile(optimizer=Adam, loss='mse')\n",
    "        return model\n",
    "\n",
    "    model =  Create_model()\n",
    "    # Train the model\n",
    "    # Create a callback that saves the model's weights\n",
    "    checkpoint_path = \"./Light_GDP/PSO_BPNN/\"+data_name+'.ckpt'\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                     #save_weights_only=True,\n",
    "                                                     save_best_only = True,\n",
    "                                                     verbose=0)\n",
    "    model.fit(train_x, \n",
    "              train_y, \n",
    "              epochs=1000, \n",
    "              batch_size=10000,\n",
    "              #validation_split=0.05,\n",
    "              validation_data=(vali_x, vali_y),\n",
    "              callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 20)],\n",
    "              verbose = 1\n",
    "             )\n",
    "    model.summary()\n",
    "    model.save('./Light_GDP/PSO_BPNN/'+data_name+'_LogX_model.h5')\n",
    "    plt.plot(model.history.history['loss'], c = 'blue')\n",
    "    plt.plot(model.history.history['val_loss'], c = 'red')\n",
    "    plt.show()\n",
    "\n",
    "    # Predition\n",
    "    pred_y = model.predict(test_x,batch_size=100000)\n",
    "\n",
    "    # plot the results\n",
    "    import matplotlib.pyplot as plt\n",
    "    # test the model\n",
    "    model.evaluate(test_x, test_y,verbose=2)\n",
    "    #print('Test loss:', test_loss)\n",
    "    #print('Test accuracy:', test_acc)\n",
    "\n",
    "    #plt.plot(train_x, train_y, '.', label='train data')\n",
    "    plt.plot(test_y, pred_y, '.', label='model prediction')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    pred_train = model.predict(train_x)\n",
    "    plt.plot(train_y, pred_train, '.', label='model prediction')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    ## Correlation coefficient\n",
    "    import scipy.stats as stats\n",
    "    print(stats.pearsonr((np.array(test_y)).reshape(-1,), (pred_y).reshape(-1,)))\n",
    "    print(stats.pearsonr((np.array(train_y)).reshape(-1,), (pred_train).reshape(-1,)))\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cad743",
   "metadata": {},
   "source": [
    "## MLP prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbb668e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import rasterio as rio\n",
    "import rasterio\n",
    "from rasterio.merge import merge\n",
    "from rasterio.mask import mask\n",
    "from rasterio import Affine\n",
    "import matplotlib.pyplot as plt\n",
    "from rasterio.plot import show\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import jenkspy\n",
    "import scipy\n",
    "from scipy.spatial import distance_matrix\n",
    "import scipy.stats as stats\n",
    "%matplotlib inline\n",
    "\n",
    "XV = ['X','Y'\n",
    "      ,'N11','N12','N13',\n",
    "      'N14','N0','N15',\n",
    "      'N16','N17','N18',\n",
    "      'N21','N22','N23','N24','N25','N26','N27','N28','N29','N210','N211','N212','N213','N214','N215','N216'\n",
    "      ]\n",
    "YV = ['VDMSP']\n",
    "continentList = [\"Africa\",\"Asia\", \"Europe\", \"South America\", \"Oceania\", \"North America\"] #\n",
    "\n",
    "metric_ste = np.zeros((6,6))\n",
    "#continentList = [\"Asia\"]\n",
    "stei = -1\n",
    "for ste in continentList:\n",
    "    \n",
    "    scaler_x = MinMaxScaler()\n",
    "    scaler_y = MinMaxScaler()\n",
    "    data_name = 'Nei2'+ste\n",
    "    for year in range(2013,2023):\n",
    "        model_ste = tf.keras.models.load_model('./Light_GDP/PSO_BPNN/'+data_name+'2013_LogX_model.h5')\n",
    "        #checkpoint_path = \"./Light_GDP/PSO_BPNN/\"+ste+'cp.ckpt'\n",
    "        #model_ste = Create_model()\n",
    "        #model_ste.load_weights(checkpoint_path)\n",
    "        #data_df = pd.read_csv('./Light_GDP/PSO_BPNN/ALLHiLoMedianLag3_odr2'+ste+MaxLimitLab+LogDataLab+'.csv',sep = ',')\n",
    "        data_df = pd.read_csv('./Light_GDP/PSO_BPNN/'+data_name+str(year)+'.csv',sep = ',')\n",
    "        X = data_df.loc[:,XV]\n",
    "        #model_ste= model\n",
    "        if year == 2013:\n",
    "            scaler_x.fit_transform(X)\n",
    "            y = data_df.loc[:,YV]\n",
    "            scaler_y.fit_transform(y)\n",
    "\n",
    "            #print(model_ste.evaluate(test_x, test_y,verbose=2))\n",
    "            \n",
    "\n",
    "        XV_log = ['N11','N12','N13'\n",
    "          ,'N14','N0','N15','N16','N17','N18'\n",
    "          ,'N21','N22','N23','N24','N25','N26','N27','N28','N29','N210','N211','N212','N213','N214','N215','N216'\n",
    "          ]\n",
    "        \n",
    "        X.loc[:,XV_log] = np.log(X.loc[:,XV_log]+1)\n",
    "\n",
    "        X = scaler_x.transform(X)\n",
    "\n",
    "        pred_y = model_ste.predict(X,batch_size=100000)\n",
    "        pred_y_ori = scaler_y.inverse_transform(pred_y)\n",
    "        data_df['DMSP_like_NPP'] = pred_y_ori.reshape(-1,)\n",
    "        #print(pred_data.VDMSP.corr(pred_data.pred_DMSP))\n",
    "        #pred_data_out = pred_data.loc[:,['rowid', 'colid', 'pred_DMSP']]\n",
    "\n",
    "        raster_npp = rio.open(\"./Light_GDP/DMSL-NOAA/NPP/Continent/NPP_NTL_MaxFilt_SaturCotinMedian_\"+str(year)+\"Ste\"+ste+\".tif\")\n",
    "        data_npp = raster_npp.read(1)\n",
    "        data_npp[data_df.rowid,data_df.colid] = data_df.DMSP_like_NPP\n",
    "        out_transform = raster_npp.transform\n",
    "        \n",
    "        if year == 2013:\n",
    "            stei+=1\n",
    "            pearson_r = stats.pearsonr((np.array(y)).reshape(-1,), (pred_y_ori).reshape(-1,))\n",
    "            R2 = pearson_r[0]**2\n",
    "            print(R2)\n",
    "            metric_ste[stei,0] = R2\n",
    "            \n",
    "        if False:\n",
    "            if year == 2013:\n",
    "                raster_dmsp = rio.open(\"./Light_GDP/DMSL-NOAA/DMSP/DMSP_NTL_SaturCotin_2013Ste\"+ste+\".tif\")\n",
    "                data_pre = raster_dmsp.read(1)\n",
    "                data_npp[data_npp<data_pre] = data_pre[data_npp<data_pre]\n",
    "                data_npp[data_npp>data_pre] = (data_pre[data_npp>data_pre]+data_npp[data_npp>data_pre])/2\n",
    "                \n",
    "            else:\n",
    "                raster_npp = rio.open(\"./Light_GDP/DMSL-NOAA/Pred_NPP/PredNPPMedianLag3\"+ste+str(year-1)+\"tf257.tif\")\n",
    "                data_pre = raster_npp.read(1)\n",
    "                data_npp[data_npp<data_pre] = data_pre[data_npp<data_pre]\n",
    "        \n",
    "        data_npp[data_npp<0] = 0\n",
    "        \n",
    "        ### save\n",
    "        out_meta = raster_npp.meta.copy()\n",
    "        out_meta.update({\"driver\": \"GTiff\", \"height\": data_npp.shape[0], \"width\": data_npp.shape[1], \"transform\": out_transform})\n",
    "\n",
    "        with rasterio.open(\"./Light_GDP/DMSL-NOAA/Pred_NPP/NeiMLPPredNPPMedian\"+ste+str(year)+\"tf257.tif\", \"w\", **out_meta,) as dest:\n",
    "            dest.write(data_npp,1)\n",
    "        print(str(year) + ': ' + ste)\n",
    "raster_npp.close()\n",
    "#raster_dmsp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5bfe08",
   "metadata": {},
   "source": [
    "### HSAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d14ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 如果 mse 是 Keras 内置的均方误差损失函数，则确保在加载模型时它是已知的\n",
    "custom_objects = {'mse': tf.keras.losses.MeanSquaredError()}\n",
    "\n",
    "# 模型保存路径\n",
    "model_path = './Light_GDP/PSO_BPNN/{}_LogX_model.h5'.format(data_name)\n",
    "\n",
    "# 加载模型\n",
    "loaded_model = tf.keras.models.load_model(model_path, custom_objects=custom_objects)\n",
    "\n",
    "# 打印模型结构\n",
    "loaded_model.summary()\n",
    "model = loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf290698",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "import shap\n",
    "\n",
    "local_shp = gpd.read_file(r'\\\\10.95.10.91\\d\\BaiduSyncdisk\\data\\Climate_Economy/Tri_State_Area_NK_Shanghai.shp')\n",
    "conti_local = {\"Africa\":'EG', \"Asia\":'BJ',\"Europe\":'LD', \"South America\":None, \"Oceania\":None, \"North America\":'NK'}\n",
    "local_shp[local_shp.Region == conti_local[ste]]\n",
    "\n",
    "gid=local_shp[local_shp.Region == conti_local[ste]].index[0]\n",
    "\n",
    "local_shp.loc[gid:gid,].plot()\n",
    "gm_1 = local_shp.geometry[gid]\n",
    "train_x.max(axis = 0)\n",
    "\n",
    "data_name = 'Nei2'+ste+'2013'\n",
    "#data_df = pd.read_csv('./Light_GDP/PSO_BPNN/ALLHiLoMedianLag3_odr2'+ste+MaxLimitLab+LogDataLab+'.csv',sep = ',')\n",
    "data_df_ste = pd.read_csv('./Light_GDP/PSO_BPNN/'+data_name+'.csv',sep = ',')\n",
    "\n",
    "minx,miny,maxx,maxy = gm_1.bounds\n",
    "data_df_ste_1 = data_df_ste[((data_df_ste.X>minx) & (data_df_ste.X<maxx)) & ((data_df_ste.Y>miny) & (data_df_ste.Y<maxy))].copy()\n",
    "\n",
    "XYV = ['VDMSP','X','Y'\n",
    "      ,'N11','N12','N13',\n",
    "      'N14','N0','N15',\n",
    "      'N16','N17','N18',\n",
    "      'N21','N22','N23','N24','N25','N26','N27','N28','N29','N210','N211','N212','N213','N214','N215','N216'\n",
    "      ]\n",
    "\n",
    "data_df_ste_1_thd = data_df_ste_1#[(data_df_ste_1.VDMSP>0) & (data_df_ste_1.VDMSP<100)]\n",
    "\n",
    "XV_log = ['N11','N12','N13'\n",
    "  ,'N14','N0','N15','N16','N17','N18'\n",
    "  ,'N21','N22','N23','N24','N25','N26','N27','N28','N29','N210','N211','N212','N213','N214','N215','N216'\n",
    "  ]\n",
    "shap_X = data_df_ste_1_thd.loc[:,XV]\n",
    "#train_y.loc[:,YV] = np.log(train_y.loc[:,YV]+1)\n",
    "shap_X.loc[:,XV_log] = np.log(shap_X.loc[:,XV_log]+1)\n",
    "\n",
    "sample_x = scaler_x.transform(shap_X)\n",
    "sample_y = scaler_y.transform(data_df_ste_1_thd.loc[ :,YV])\n",
    "\n",
    "background_sample = sample_x\n",
    "\n",
    "\n",
    "# Interpreting the model using SHAP\n",
    "explainer = shap.Explainer(model, background_sample)  # Calculate the background distribution using part of the training data\n",
    "test_sample = background_sample#resample(sample_x, replace=False, n_samples=10000, random_state=42)\n",
    "shap_values = explainer.shap_values(test_sample)  # Interpreting the data\n",
    "\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "background_sample_inv = scaler_x.inverse_transform(test_sample)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fa9f6b",
   "metadata": {},
   "source": [
    "Feature Importance Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314055b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "\n",
    "\n",
    "# Create a custom color ramp from blue to red\n",
    "def rgb_to_hex(rgb):\n",
    "    \"\"\"Converts RGB tuple to hexadecimal color string.\"\"\"\n",
    "    return '#{:02x}{:02x}{:02x}'.format(*rgb)\n",
    "\n",
    "rgb_tuple = (255, 0, 0) \n",
    "hex_color1 = rgb_to_hex((0,138,252))\n",
    "hex_color2 = rgb_to_hex((255,0,82))\n",
    "\n",
    "colors = [hex_color1, hex_color2]\n",
    "nodes = [0.0, 1.0]  \n",
    "cmap_name = \"blue_red\"\n",
    "custom_cmap = LinearSegmentedColormap.from_list(cmap_name, list(zip(nodes, colors)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "shap.summary_plot(shap_values, test_sample,max_display = 35, feature_names=[f'{i}' for i in XV_name], plot_type='dot', show=False, layered_violin_max_num_bins=27)\n",
    "\n",
    "\n",
    "plt.savefig(out_path + 'shap_summary_plot_africa_'+conti_local[ste]+'.jpg',bbox_inches='tight',pad_inches=0,dpi = 600,)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179aff1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea95d74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13056641",
   "metadata": {},
   "source": [
    "## LightGBM for DMSP-like conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc968148",
   "metadata": {},
   "source": [
    "Do a grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155bd420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import multiprocessing\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import scipy.stats as stats\n",
    "\n",
    "import os\n",
    "\n",
    "wksp = r'\\\\10.95.10.91\\d/BaiduSyncdisk/data/Climate_Economy/'\n",
    "os.chdir(wksp)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ipyleaflet import Map, Marker\n",
    "import geopandas as gpd\n",
    "import rasterio as rio\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import convolve2d\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from multiprocessing import Process, Manager\n",
    "\n",
    "import lightgbm as lgb\n",
    "import sklearn\n",
    "import time\n",
    "import jenkspy\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_and_evaluate(params, ttv_datas_all, results_list, mod_i):\n",
    "    print(mod_i,'started')\n",
    "    objective, max_depth, num_leaves, subsample, min_child_samples, min_child_weight, learning_rate,reg_alpha, reg_lambda = params\n",
    "    train_x_ht_all, train_y_ht_all, test_x_ht_all, test_y_ht_all, vali_x_ht_all, vali_y_ht_all = ttv_datas_all\n",
    "    start_time = time.time()\n",
    "    #建立LGB的dataset格式数据\n",
    "    lgb_train = lgb.Dataset(train_x_ht_all, train_y_ht_all)\n",
    "    lgb_eval = lgb.Dataset(vali_x_ht_all, vali_y_ht_all, reference=lgb_train)\n",
    "\n",
    "    params = {'task': 'train',\n",
    "                 'boosting_type': 'gbdt',\n",
    "                 'feature_fraction': 0.8,\n",
    "                 'bagging_fraction': 0.8,\n",
    "                 'bagging_freq': 5,\n",
    "                 'verbose': 1,\n",
    "                 'subsample_freq': 5,\n",
    "                 'n_estimators':10000,\n",
    "                 'objective': objective,\n",
    "                 'metric': ['mae'],\n",
    "                 'learning_rate': learning_rate,\n",
    "                 'max_depth': max_depth,\n",
    "                 'num_leaves': num_leaves,\n",
    "                 'min_child_samples': min_child_samples,\n",
    "                 'min_child_weight': min_child_weight,\n",
    "                 'colsample_bytree': 0.9,\n",
    "                 'subsample': subsample,\n",
    "                 'reg_alpha': reg_alpha,\n",
    "                 'reg_lambda': reg_lambda}\n",
    "    #定义callback回调\n",
    "    callback=[lgb.early_stopping(stopping_rounds=10,verbose=True),\n",
    "              lgb.log_evaluation(period=10,show_stdv=True)]\n",
    "    # 训练 train\n",
    "    m1 = lgb.train(params,lgb_train,num_boost_round=10000,\n",
    "                   valid_sets=[lgb_train,lgb_eval],callbacks=callback)\n",
    "\n",
    "    training_time = time.time() - start_time\n",
    "    train_loss = list(m1.best_score['training'].values())[0]\n",
    "    val_lose = list(m1.best_score['valid_1'].values())[0]\n",
    "    #预测数据集\n",
    "\n",
    "    pred_y_test = m1.predict(test_x_ht_all, batch_size=100000)\n",
    "    R_test = stats.pearsonr((np.array(test_y_ht_all)).reshape(-1, ), (pred_y_test).reshape(-1, ))[0]\n",
    "\n",
    "    pred_y_train = m1.predict(train_x_ht_all, batch_size=100000)\n",
    "    R_train = stats.pearsonr((np.array(train_y_ht_all)).reshape(-1, ), (pred_y_train).reshape(-1, ))[0]\n",
    "\n",
    "    pred_y_vali = m1.predict(vali_x_ht_all, batch_size=100000)\n",
    "    R_vali = stats.pearsonr((np.array(vali_y_ht_all)).reshape(-1, ), (pred_y_vali).reshape(-1, ))[0]\n",
    "\n",
    "\n",
    "    print(f\"Params: {params}, Loss: {train_loss:.4f},R:{R_test:.4f}, MAE: {val_lose:.4f}, Time: {training_time:.2f} sec\")\n",
    "    print(mod_i, 'finished')\n",
    "    results_list.append((params,  R_test, R_train, R_vali, train_loss, val_lose, training_time))\n",
    "\n",
    "\n",
    "mod_i = 0\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    continentList = [\"Africa\", \"Asia\", \"Europe\", \"South America\", \"Oceania\", \"North America\"]  #\n",
    "    # 打开栅格文件\n",
    "    # continentList = [\"Africa\"]\n",
    "    train_x_ht_all = np.array([]).reshape(0, 27)\n",
    "    train_y_ht_all = np.array([]).reshape(0, 1)\n",
    "\n",
    "    test_x_ht_all = np.array([]).reshape(0, 27)\n",
    "    test_y_ht_all = np.array([]).reshape(0, 1)\n",
    "\n",
    "    vali_x_ht_all = np.array([]).reshape(0, 27)\n",
    "    vali_y_ht_all = np.array([]).reshape(0, 1)\n",
    "\n",
    "    for ste in (continentList):\n",
    "        print(ste)\n",
    "        data_name = 'Nei2' + ste + '2013'\n",
    "        # data_df = pd.read_csv('./Light_GDP/PSO_BPNN/ALLHiLoMedianLag3_odr2'+ste+MaxLimitLab+LogDataLab+'.csv',sep = ',')\n",
    "        data_df = pd.read_csv('./Light_GDP/PSO_BPNN/' + data_name + '.csv', sep=',')\n",
    "\n",
    "        data_df.head\n",
    "\n",
    "        # 设置随机数种子\n",
    "        tf.random.set_seed(42)\n",
    "\n",
    "        # 生成训练数据\n",
    "\n",
    "        # XV = ['X','Y','lagnpp','lagnpp2','DistSat', 'VNPP']\n",
    "        XV = ['X', 'Y'\n",
    "            , 'N11', 'N12', 'N13',\n",
    "              'N14', 'N0', 'N15',\n",
    "              'N16', 'N17', 'N18',\n",
    "              'N21', 'N22', 'N23', 'N24', 'N25', 'N26', 'N27', 'N28', 'N29', 'N210', 'N211', 'N212', 'N213', 'N214',\n",
    "              'N215',\n",
    "              'N216'\n",
    "              ]\n",
    "        YV = ['VDMSP']\n",
    "        #\n",
    "\n",
    "        data_len = data_df.shape[0]\n",
    "        # train_len = int(data_df.shape[0]*0.6)\n",
    "        scaler_x = MinMaxScaler()\n",
    "        scaler_y = MinMaxScaler()\n",
    "        # train_x = np.array(data_df.VNPP[0:train_len]).reshape(-1,1)\n",
    "        # train_y = np.array(data_df.VDMSP[0:train_len]).reshape(-1,1)\n",
    "        # train_x, test_x, train_y, test_y = train_test_split(data_df.loc[:,['X','Y','lagnpp','lagnpp2','DistSat','VNPP']], data_df.loc[:,['VDMSP']], test_size=0.4, random_state=42)\n",
    "\n",
    "        # train_x, test_x, train_y, test_y = train_test_split(data_df.loc[:,XV], data_df.loc[:,['VDMSP']], test_size=0.4, random_state=42)\n",
    "        # 分段采样\n",
    "        breaks_dic = {\n",
    "            'Africa': [-1, 7.057890251, 26.21502093, 53.4383119, 85.70295305, 120.9924043, 156.2818556, 188.5464967,\n",
    "                       220.8111378, 300],\n",
    "            'Asia': [-1, 6.049620215, 25.2067509, 53.4383119, 85.70295305, 120.9924043, 156.2818556, 189.5547667,\n",
    "                     220.8111378, 300],\n",
    "            'Europe': [-1, 8.066160287, 28.231561, 56.46312201, 89.73603319, 126.0337545, 160.3149357, 193.5878469,\n",
    "                       226.8607581, 300],\n",
    "            'North America': [-1, 7.057890251, 26.21502093, 54.44658194, 88.72776315, 126.0337545, 162.3314758,\n",
    "                              197.620927,\n",
    "                              231.9021082, 300],\n",
    "            'Oceania': [-1, 6.049620215, 25.2067509, 54.44658194, 89.73603319, 126.0337545, 162.3314758, 197.620927,\n",
    "                        230.8938382, 300],\n",
    "            'South America': [-1, 6.049620215, 25.2067509, 53.4383119, 85.70295305, 120.9924043, 157.2901256,\n",
    "                              192.5795768,\n",
    "                              227.8690281, 300]\n",
    "        }\n",
    "\n",
    "        breaks = breaks_dic[ste]\n",
    "        if 'Log' in data_name:\n",
    "            breaks = np.log(breaks)\n",
    "            breaks[0] = -100\n",
    "\n",
    "        vi_train = np.array([])\n",
    "        vi_test = np.array([])\n",
    "        vi_vali = np.array([])\n",
    "        len(breaks)\n",
    "        import numpy as np\n",
    "\n",
    "        total = 9\n",
    "        max_quantile = 0.5\n",
    "        quants = np.linspace(1, total, total)\n",
    "        quants = np.power(quants, 2)\n",
    "        quants = quants / (total ** 2) * max_quantile\n",
    "        print(quants)\n",
    "\n",
    "        for bki in range(1, len(breaks)):\n",
    "\n",
    "            vi = np.arange(0, data_df.shape[0])\n",
    "            vi_inbk = vi[np.logical_and(data_df[YV[0]] > breaks[bki - 1], data_df[YV[0]] <= breaks[bki])]\n",
    "            vi_inbk_train, vi_inbk_test = train_test_split(vi_inbk, test_size=0.2)\n",
    "\n",
    "            if False:  # Remove noise by distribution\n",
    "                VNPP_tmp = data_df.loc[vi_inbk_train, ['VNPP']]\n",
    "                VDMSP_tmp = data_df.loc[vi_inbk_train, ['VDMSP']]\n",
    "                VNPP_Z = (VNPP_tmp - np.mean(VNPP_tmp)) / np.std(VNPP_tmp)\n",
    "                vi_inbk_train = vi_inbk_train[np.array(np.logical_and(VNPP_Z > -3, VNPP_Z < 3)).reshape(-1, )]\n",
    "\n",
    "                VNPP_tmp_test = data_df.loc[vi_inbk_test, ['VNPP']]\n",
    "                VDMSP_tmp_test = data_df.loc[vi_inbk_test, ['VDMSP']]\n",
    "\n",
    "                VNPP_Z_test = (VNPP_tmp_test - np.mean(VNPP_tmp_test)) / np.std(VNPP_tmp_test)\n",
    "                vi_inbk_test = vi_inbk_test[np.array(np.logical_and(VNPP_Z_test > -3, VNPP_Z_test < 3)).reshape(-1, )]\n",
    "\n",
    "            if False:  # Remove Noise by quantiale\n",
    "\n",
    "                VNPP_tmp = data_df.loc[vi_inbk_train, ['VNPP']]\n",
    "                VDMSP_tmp = data_df.loc[vi_inbk_train, ['VDMSP']]\n",
    "\n",
    "                vi_inbk_train = vi_inbk_train[\n",
    "                    np.array(np.logical_and(VNPP_tmp >= np.quantile(VNPP_tmp, [np.exp(-(10 - bki)), 0.95])[0],\n",
    "                                            1)).reshape(-1, )]\n",
    "                VNPP_tmp_test = data_df.loc[vi_inbk_test, ['VNPP']]\n",
    "                VDMSP_tmp_test = data_df.loc[vi_inbk_test, ['VDMSP']]\n",
    "                vi_inbk_test = vi_inbk_test[\n",
    "                    np.array(np.logical_and(VNPP_tmp_test >= np.quantile(VNPP_tmp_test, [np.exp(-(10 - bki)), 0.95])[0],\n",
    "                                            1)).reshape(-1, )]\n",
    "\n",
    "            vi_inbk_train, vi_inbk_vali = train_test_split(vi_inbk_train, test_size=0.3)\n",
    "            vi_train = np.append(vi_train, vi_inbk_train)\n",
    "            vi_test = np.append(vi_test, vi_inbk_test)\n",
    "            vi_vali = np.append(vi_vali, vi_inbk_vali)\n",
    "\n",
    "        # train_x, test_x, train_y, test_y = train_test_split(data_df.loc[:,XV], data_df.loc[:,['VDMSP']], test_size=0.4, random_state=42)\n",
    "\n",
    "        train_x = data_df.loc[vi_train, XV]\n",
    "        train_y = data_df.loc[vi_train, YV]\n",
    "\n",
    "        test_x = data_df.loc[vi_test, XV]\n",
    "        test_y = data_df.loc[vi_test, YV]\n",
    "\n",
    "        vali_x = data_df.loc[vi_vali, XV]\n",
    "        vali_y = data_df.loc[vi_vali, YV]\n",
    "\n",
    "        # np.save('./Light_GDP/PSO_BPNN/model_save/MLP_'+ste+'_vi_train.npy',vi_train)\n",
    "        # np.save('./Light_GDP/PSO_BPNN/model_save/MLP_'+ste+'_vi_vali.npy',vi_vali)\n",
    "        # np.save('./Light_GDP/PSO_BPNN/model_save/MLP_'+ste+'_vi_test.npy',vi_test)\n",
    "\n",
    "        if True:  # Is or Not log : False for better\n",
    "\n",
    "            XV_log = ['N11', 'N12', 'N13'\n",
    "                , 'N14', 'N0', 'N15', 'N16', 'N17', 'N18'\n",
    "                , 'N21', 'N22', 'N23', 'N24', 'N25', 'N26', 'N27', 'N28', 'N29', 'N210', 'N211', 'N212', 'N213', 'N214',\n",
    "                      'N215', 'N216'\n",
    "                      ]\n",
    "\n",
    "            # train_y.loc[:,YV] = np.log(train_y.loc[:,YV]+1)\n",
    "            train_x.loc[:, XV_log] = np.log(train_x.loc[:, XV_log] + 1)\n",
    "            # vali_y.loc[:,YV] = np.log(vali_y.loc[:,YV]+1)\n",
    "            vali_x.loc[:, XV_log] = np.log(vali_x.loc[:, XV_log] + 1)\n",
    "            # test_y.loc[:,YV] = np.log(test_y.loc[:,YV]+1)\n",
    "            test_x.loc[:, XV_log] = np.log(test_x.loc[:, XV_log] + 1)\n",
    "\n",
    "        if True:  # Is or Not Normalization : Is for better or worse?\n",
    "            scaler_x.fit_transform(data_df.loc[:, XV])\n",
    "            scaler_y.fit_transform(data_df.loc[:, YV])\n",
    "            train_x = scaler_x.transform(train_x)\n",
    "            train_y = scaler_y.transform(train_y)\n",
    "            test_x = scaler_x.transform(test_x)\n",
    "            test_y = scaler_y.transform(test_y)\n",
    "            vali_x = scaler_x.transform(vali_x)\n",
    "            vali_y = scaler_y.transform(vali_y)\n",
    "        t, train_x_ht, t1, train_y_ht = train_test_split(train_x, train_y, test_size=0.01, random_state=2)\n",
    "        t, test_x_ht, t1, test_y_ht = train_test_split(test_x, test_y, test_size=0.01, random_state=2)\n",
    "        t, vali_x_ht, t1, vali_y_ht = train_test_split(vali_x, vali_y, test_size=0.01, random_state=2)\n",
    "\n",
    "        train_x_ht_all = np.concatenate([train_x_ht_all, train_x_ht])\n",
    "        train_y_ht_all = np.concatenate([train_y_ht_all, train_y_ht])\n",
    "        test_x_ht_all = np.concatenate([test_x_ht_all, test_x_ht])\n",
    "        test_y_ht_all = np.concatenate([test_y_ht_all, test_y_ht])\n",
    "        vali_x_ht_all = np.concatenate([vali_x_ht_all, vali_x_ht])\n",
    "        vali_y_ht_all = np.concatenate([vali_y_ht_all, vali_y_ht])\n",
    "\n",
    "    # Set up workspace and other configurations\n",
    "    # Data loading and preprocessing\n",
    "    # Define param_grid\n",
    "    # Setup Manager and results_list\n",
    "    # 定义超参数范围\n",
    "\n",
    "    param_distributions = {\n",
    "        'objective': ['regression_l2', 'poisson', 'regression_l1'],#\n",
    "        'max_depth': [20, 30, 40, 50],\n",
    "        'num_leaves': [20, 40, 60, 80],\n",
    "        'subsample': [0.8, 0.9],\n",
    "        'min_child_samples': [18, 19, 20, 21, 22],\n",
    "        'min_child_weight': [0.001],\n",
    "        'learning_rate': [0.001, 0.01],#\n",
    "        'reg_alpha': [0.001],\n",
    "        'reg_lambda': [0.3]\n",
    "    }\n",
    "    # 生成参数组合\n",
    "    param_combinations = list(itertools.product(\n",
    "        param_distributions['objective'],\n",
    "        param_distributions['max_depth'],\n",
    "        param_distributions['num_leaves'],\n",
    "        param_distributions['subsample'],\n",
    "        param_distributions['min_child_samples'],\n",
    "        param_distributions['min_child_weight'],\n",
    "        param_distributions['learning_rate'],\n",
    "        param_distributions['reg_alpha'],\n",
    "        param_distributions['reg_lambda']\n",
    "\n",
    "    ))\n",
    "\n",
    "    ttv_datas_all = [train_x_ht_all, train_y_ht_all,test_x_ht_all,test_y_ht_all,vali_x_ht_all,vali_y_ht_all]\n",
    "    # 使用多进程处理\n",
    "    manager = Manager()\n",
    "    results_list = manager.list()\n",
    "    processes_all = []\n",
    "    processes = []\n",
    "    # Start multiprocessing\n",
    "    for params in param_combinations:\n",
    "        mod_i+=1\n",
    "        print(mod_i,'initialization')\n",
    "        p = Process(target=train_and_evaluate, args=(params, ttv_datas_all, results_list,mod_i))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "        processes_all.append(p)\n",
    "        if len(processes) > 0:\n",
    "            for p in processes:\n",
    "                p.join()\n",
    "            processes = []\n",
    "\n",
    "    for p in processes_all:\n",
    "        p.join()\n",
    "\n",
    "    # 输出结果\n",
    "    for result in results_list:\n",
    "        print(result)\n",
    "    print((results_list))\n",
    "    results_list_df = pd.DataFrame(results_list[0:])\n",
    "    results_list_df.columns = ['params', 'R_test', 'R_train', 'R_vali', 'train_loss', 'val_loss', 'training_time']\n",
    "    results_list_df.to_csv(r'\\\\10.95.10.91\\d\\BaiduSyncdisk\\data\\Climate_Economy\\Light_GDP\\revised1_results/HT_results_list_01_LightGBM_RRR.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ee04556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import sklearn\n",
    "import time\n",
    "import jenkspy\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e90a11a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metric_ste = np.zeros((6,6))\n",
    "continentList = [\"Africa\", \"Asia\",\"Europe\", \"South America\", \"Oceania\", \"North America\"] # \n",
    "\n",
    "stei = -1\n",
    "for ste in continentList:\n",
    "    stei +=1\n",
    "    data_name = 'Nei2'+ste+'2013'\n",
    "    data_df = pd.read_csv('./Light_GDP/PSO_BPNN/'+data_name+'.csv',sep = ',')\n",
    "\n",
    "    data_df.head\n",
    "\n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "    if 'Nei' not in data_name:\n",
    "        XV = ['X','Y','lagnpp','lagnpp2', 'VNPP']\n",
    "        YV = ['VDMSP'] # 'VDMSP'\n",
    "    else:\n",
    "        XV = ['N11','N12','N13'\n",
    "          ,'N14','N0','N15','N16','N17','N18'\n",
    "          ,'N21','N22','N23','N24','N25','N26','N27','N28','N29','N210','N211','N212','N213','N214','N215','N216',\n",
    "            'X','Y'\n",
    "          ]\n",
    "        YV = ['VDMSP'] # \n",
    "        \n",
    "    data_len = data_df.shape[0]\n",
    "    #train_len = int(data_df.shape[0]*0.6)\n",
    "    scaler_x = MinMaxScaler()\n",
    "    scaler_y = MinMaxScaler()\n",
    "    breaks_dic = {'Africa':[-1,7.057890251,26.21502093,53.4383119,85.70295305,120.9924043,156.2818556,188.5464967,220.8111378,300],\n",
    "              'Asia':[-1,6.049620215,25.2067509,53.4383119,85.70295305,120.9924043,156.2818556,189.5547667,220.8111378,300],\n",
    "              'Europe':[-1,8.066160287,28.231561,56.46312201,89.73603319,126.0337545,160.3149357,193.5878469,226.8607581,300],\n",
    "              'North America':[-1,7.057890251,26.21502093,54.44658194,88.72776315,126.0337545,162.3314758,197.620927,231.9021082,300],\n",
    "              'Oceania':[-1,6.049620215,25.2067509,54.44658194,89.73603319,126.0337545,162.3314758,197.620927,230.8938382,300],\n",
    "              'South America':[-1,6.049620215,25.2067509,53.4383119,85.70295305,120.9924043,157.2901256,192.5795768,227.8690281,300]\n",
    "             }\n",
    "\n",
    "    breaks = breaks_dic[ste]\n",
    "    if 'Log' in data_name:\n",
    "        breaks = np.log(breaks)\n",
    "        breaks[0] = -100\n",
    "\n",
    "    vi_train = np.array([])\n",
    "    vi_test = np.array([])\n",
    "    vi_vali = np.array([])\n",
    "    len(breaks)\n",
    "    import numpy as np\n",
    "\n",
    "    if True:\n",
    "        for bki in range(1,len(breaks)):\n",
    "\n",
    "            vi = np.arange(0,data_df.shape[0])\n",
    "            vi_inbk = vi[np.logical_and(data_df[YV[0]] > breaks[bki-1], data_df[YV[0]] <= breaks[bki])]\n",
    "            vi_inbk_train, vi_inbk_test = train_test_split(vi_inbk, test_size=0.2)\n",
    "\n",
    "            if False: # Remove noise by distribution: test\n",
    "                VNPP_tmp = data_df.loc[vi_inbk_train,['VNPP']]\n",
    "                VDMSP_tmp = data_df.loc[vi_inbk_train,['VDMSP']]\n",
    "                VNPP_Z = (VNPP_tmp - np.mean(VNPP_tmp))/np.std(VNPP_tmp)\n",
    "                vi_inbk_train = vi_inbk_train[np.array(np.logical_and(VNPP_Z> -3,VNPP_Z < 3 )).reshape(-1,)]\n",
    "\n",
    "                VNPP_tmp_test = data_df.loc[vi_inbk_test,['VNPP']]\n",
    "                VDMSP_tmp_test = data_df.loc[vi_inbk_test,['VDMSP']]\n",
    "\n",
    "                VNPP_Z_test = (VNPP_tmp_test - np.mean(VNPP_tmp_test))/np.std(VNPP_tmp_test)\n",
    "                vi_inbk_test = vi_inbk_test[np.array(np.logical_and(VNPP_Z_test> -3,VNPP_Z_test < 3 )).reshape(-1,)]\n",
    "\n",
    "            if False:# Remove Noise by quantiale: test\n",
    "\n",
    "                VNPP_tmp = data_df.loc[vi_inbk_train,['VNPP']]\n",
    "                VDMSP_tmp = data_df.loc[vi_inbk_train,['VDMSP']]\n",
    "\n",
    "                vi_inbk_train = vi_inbk_train[np.array(np.logical_and(VNPP_tmp>= np.quantile(VNPP_tmp,[np.exp(-(10-bki)),0.95])[0],\n",
    "                                                                      1)).reshape(-1,)]\n",
    "                VNPP_tmp_test = data_df.loc[vi_inbk_test,['VNPP']]\n",
    "                VDMSP_tmp_test = data_df.loc[vi_inbk_test,['VDMSP']]\n",
    "                vi_inbk_test = vi_inbk_test[np.array(np.logical_and(VNPP_tmp_test>= np.quantile(VNPP_tmp_test,[np.exp(-(10-bki)),0.95])[0],\n",
    "                                                                      1 )).reshape(-1,)]\n",
    "\n",
    "            vi_inbk_train, vi_inbk_vali = train_test_split(vi_inbk_train, test_size=0.3)\n",
    "            vi_train = np.append(vi_train, vi_inbk_train)\n",
    "            vi_test = np.append(vi_test, vi_inbk_test)\n",
    "            vi_vali = np.append(vi_vali, vi_inbk_vali)\n",
    "            \n",
    "    else: #: test\n",
    "        vi = data_df.index\n",
    "        vi_train, vi_test = train_test_split(vi, test_size=0.2)      \n",
    "        vi_train, vi_vali = train_test_split(vi_train, test_size=0.1)\n",
    "        \n",
    "    #train_x, test_x, train_y, test_y = train_test_split(data_df.loc[:,XV], data_df.loc[:,['VDMSP']], test_size=0.4, random_state=42)\n",
    "    \n",
    "    train_x = data_df.loc[vi_train,XV]\n",
    "    train_y = data_df.loc[vi_train,YV]\n",
    "\n",
    "    test_x = data_df.loc[vi_test,XV]\n",
    "    test_y = data_df.loc[vi_test,YV]\n",
    "\n",
    "    vali_x = data_df.loc[vi_vali,XV]\n",
    "    vali_y = data_df.loc[vi_vali,YV]\n",
    "    \n",
    "    np.save('./Light_GDP/PSO_BPNN/model_save/LtGBM_'+ste+'_vi_train.npy',vi_train)\n",
    "    np.save('./Light_GDP/PSO_BPNN/model_save/LtGBM_'+ste+'_vi_vali.npy',vi_vali)\n",
    "    np.save('./Light_GDP/PSO_BPNN/model_save/LtGBM_'+ste+'_vi_test.npy',vi_test)\n",
    "\n",
    "    if True: # Is or Not log : False for better \n",
    "        \n",
    "        XV_log = ['N11','N12','N13'\n",
    "          ,'N14','N0','N15','N16','N17','N18'\n",
    "          ,'N21','N22','N23','N24','N25','N26','N27','N28','N29','N210','N211','N212','N213','N214','N215','N216'\n",
    "          ]\n",
    "        #train_y.loc[:,YV] = np.log(train_y.loc[:,YV]+1)\n",
    "        train_x.loc[:,XV_log] = np.log(train_x.loc[:,XV_log]+1)\n",
    "        #vali_y.loc[:,YV] = np.log(vali_y.loc[:,YV]+1)\n",
    "        vali_x.loc[:,XV_log] = np.log(vali_x.loc[:,XV_log]+1)\n",
    "        #test_y.loc[:,YV] = np.log(test_y.loc[:,YV]+1)\n",
    "        test_x.loc[:,XV_log] = np.log(test_x.loc[:,XV_log]+1)\n",
    "    \n",
    "        \n",
    "        \n",
    "\n",
    "    if False: # Is or Not Normalization : Is for better or worse?\n",
    "        scaler_x.fit_transform(data_df.loc[:,XV])\n",
    "        scaler_y.fit_transform(data_df.loc[ :,YV])\n",
    "        train_x = scaler_x.transform(train_x)\n",
    "        train_y = scaler_y.transform(train_y)\n",
    "        test_x = scaler_x.transform(test_x)\n",
    "        test_y = scaler_y.transform(test_y)\n",
    "        vali_x = scaler_x.transform(vali_x)\n",
    "        vali_y = scaler_y.transform(vali_y)\n",
    "\n",
    "\n",
    "    \n",
    "    def median_absolute_percentage_error(y_true,y_pred):\n",
    "        return np.median(np.abs((y_pred-y_true)/y_true))\n",
    "    \n",
    "    def regression_metrics(true,pred,pred_train,train_y,stei):\n",
    "        print('Regression model evaluation index results:')\n",
    "        #mse = sklearn.metrics.mean_squared_error(true, pred)\n",
    "        #rmse = np.sqrt(sklearn.metrics.mean_squared_error(true,pred))\n",
    "        #mae = sklearn.metrics.mean_absolute_error(true,pred)\n",
    "        #MedianAE = sklearn.metrics.median_absolute_error(true,pred)\n",
    "        #mape = sklearn.metrics.mean_absolute_percentage_error(true,pred)\n",
    "        r2 = sklearn.metrics.r2_score(true,pred)\n",
    "        r2_train = sklearn.metrics.r2_score(train_y,pred_train)\n",
    "\n",
    "        print('R squared:', r2)\n",
    "        #metric_ste[stei, :] = np.array([mse, rmse, mae, MedianAE, mape, r2])\n",
    "        metric_ste[stei, 0] = r2\n",
    "        metric_ste[stei, 1] = r2_train\n",
    "\n",
    "\n",
    "    ### Create LGB dataset format data\n",
    "    lgb_train = lgb.Dataset(train_x, train_y)\n",
    "    lgb_eval = lgb.Dataset(vali_x, vali_y, reference=lgb_train)\n",
    "    if True:\n",
    "        params = {'task': 'train',\n",
    "                     'boosting_type': 'gbdt',\n",
    "                     'feature_fraction': 0.8,\n",
    "                     'bagging_fraction': 0.8,\n",
    "                     'bagging_freq': 5,\n",
    "                     'verbose': 1,\n",
    "                     'subsample_freq': 5,\n",
    "                     'n_estimators':10000,\n",
    "                     'objective': 'regression_l2',\n",
    "                     'metric': ['mae'],\n",
    "                     'learning_rate': 0.01,\n",
    "                     'max_depth': 40,\n",
    "                     'num_leaves': 80,\n",
    "                     'min_child_samples': 20,\n",
    "                     'min_child_weight': 0.001,\n",
    "                     'colsample_bytree': 0.9,\n",
    "                     'subsample': 0.9,\n",
    "                     'reg_alpha': 0.001,\n",
    "                     'reg_lambda': 0.3}\n",
    "    else:\n",
    "        # hyperparameters\n",
    "        params = {\n",
    "            'task': 'train',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'feature_fraction': 0.8,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'bagging_freq': 5,\n",
    "            'verbose': 1,\n",
    "            'subsample_freq' : 5,\n",
    "\n",
    "            #'objective':objective,\n",
    "            'objective':'poisson',#'regression_l2',#search.best_params_['objective'],#'poisson',#,\n",
    "            'metric': 'mae',\n",
    "            'learning_rate': 0.01,\n",
    "            'max_depth':10, #search.best_params_['max_depth'],# \n",
    "            'num_leaves': 100,#search.best_params_['num_leaves'],#\n",
    "            'min_child_samples': search.best_params_['min_child_samples'],\n",
    "            'min_child_weight': search.best_params_['min_child_weight'],\n",
    "            'colsample_bytree': search.best_params_['colsample_bytree'],\n",
    "            'subsample': search.best_params_['subsample'],\n",
    "            'reg_alpha': search.best_params_['reg_alpha'],\n",
    "            'reg_lambda': search.best_params_['reg_lambda']\n",
    "\n",
    "\n",
    "        }\n",
    "    ### Define the callback callback\n",
    "    callback=[lgb.early_stopping(stopping_rounds=10,verbose=True),\n",
    "              lgb.log_evaluation(period=10,show_stdv=True)]\n",
    "    # train\n",
    "    m1 = lgb.train(params,lgb_train,num_boost_round=10000,\n",
    "                   valid_sets=[lgb_train,lgb_eval],callbacks=callback)\n",
    "    m1.save_model('./LtGBM_model'+data_name+'_LogX.txt')\n",
    "\n",
    "    \n",
    "\n",
    "    y_pred = m1.predict(test_x)\n",
    "    \n",
    "    pred_train = m1.predict(train_x)\n",
    "    # metrics\n",
    "    regression_metrics(test_y,y_pred,pred_train,train_y, stei)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae68e59c",
   "metadata": {},
   "source": [
    "### SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "18d7ef6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'Nei2'+ste+'2013'\n",
    "#data_df = pd.read_csv('./Light_GDP/PSO_BPNN/ALLHiLoMedianLag3_odr2'+ste+MaxLimitLab+LogDataLab+'.csv',sep = ',')\n",
    "data_df_ste = pd.read_csv('./Light_GDP/PSO_BPNN/'+data_name+'.csv',sep = ',')\n",
    "from sklearn.utils import resample\n",
    "import shap\n",
    "\n",
    "local_shp = gpd.read_file(r'\\\\10.95.10.91\\d\\BaiduSyncdisk\\data\\Climate_Economy/Tri_State_Area_NK_Shanghai.shp')\n",
    "\n",
    "\n",
    "conti_local = {\"Africa\":'ZWE_HA', \"Asia\":'SH',\"Europe\":'LD', \"South America\":None, \"Oceania\":None, \"North America\":'NK'}\n",
    "gid=local_shp[local_shp.Region == conti_local[ste]].index[0]\n",
    "local_shp.loc[gid:gid,].plot()\n",
    "gm_1 = local_shp.geometry[gid]\n",
    "train_x.max(axis = 0)\n",
    "minx,miny,maxx,maxy = gm_1.bounds\n",
    "local_shp[local_shp.Region == conti_local[ste]]\n",
    "\n",
    "data_df_ste_1 = data_df_ste[((data_df_ste.X>minx) & (data_df_ste.X<maxx)) & ((data_df_ste.Y>miny) & (data_df_ste.Y<maxy))].copy()\n",
    "XYV = ['VDMSP','X','Y'\n",
    "      ,'N11','N12','N13',\n",
    "      'N14','N0','N15',\n",
    "      'N16','N17','N18',\n",
    "      'N21','N22','N23','N24','N25','N26','N27','N28','N29','N210','N211','N212','N213','N214','N215','N216'\n",
    "      ]\n",
    "\n",
    "\n",
    "data_df_ste_1_thd = data_df_ste_1#[(data_df_ste_1.VDMSP>0) & (data_df_ste_1.VDMSP<100)]\n",
    "print(data_df_ste_1_thd.shape)\n",
    "XV = ['N11','N12','N13'\n",
    "  ,'N14','N0','N15','N16','N17','N18'\n",
    "  ,'N21','N22','N23','N24','N25','N26','N27','N28','N29','N210','N211','N212','N213','N214','N215','N216',\n",
    "    'X','Y'\n",
    "  ]\n",
    "YV = ['VDMSP'] # \n",
    "\n",
    "XV_log = ['N11','N12','N13'\n",
    "  ,'N14','N0','N15','N16','N17','N18'\n",
    "  ,'N21','N22','N23','N24','N25','N26','N27','N28','N29','N210','N211','N212','N213','N214','N215','N216'\n",
    "  ]\n",
    "shap_X = data_df_ste_1_thd.loc[:,XV]\n",
    "#train_y.loc[:,YV] = np.log(train_y.loc[:,YV]+1)\n",
    "shap_X.loc[:,XV_log] = np.log(shap_X.loc[:,XV_log]+1)\n",
    "shap_Y = data_df_ste_1_thd.loc[:,YV]\n",
    "\n",
    "# Interpreting the model using SHAP\n",
    "background_sample = shap_X#resample(shap_X, n_samples=1000, replace=False,random_state=42)\n",
    "\n",
    "\n",
    "explainer_lgd = shap.TreeExplainer(m1,background_sample)  # Calculate the background distribution using part of the training data\n",
    "test_sample = background_sample# resample(background_sample, n_samples=10000,replace=False, random_state=42)\n",
    "shap_values_lgd = explainer_lgd.shap_values(test_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fab9d0e",
   "metadata": {},
   "source": [
    "Feature Importance Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f27620b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "# Create a custom color ramp from blue to red\n",
    "\n",
    "\n",
    "def rgb_to_hex(rgb):\n",
    "    \"\"\"Converts RGB tuple to hexadecimal color string.\"\"\"\n",
    "    return '#{:02x}{:02x}{:02x}'.format(*rgb)\n",
    "\n",
    "\n",
    "rgb_tuple = (255, 0, 0)  \n",
    "hex_color1 = rgb_to_hex((0,138,252))\n",
    "hex_color2 = rgb_to_hex((255,0,82))\n",
    "\n",
    "colors = [hex_color1, hex_color2]\n",
    "nodes = [0.0, 1.0]  \n",
    "cmap_name = \"blue_red\"\n",
    "custom_cmap = LinearSegmentedColormap.from_list(cmap_name, list(zip(nodes, colors)))\n",
    "\n",
    "# Draw a summary of the SHAP values ​​and remove all text\n",
    "fig, ax = plt.subplots()\n",
    "shap.summary_plot(shap_values_lgd, test_sample,max_display = 35, feature_names=[f'{i}' for i in XV_name], plot_type='dot', show=False, layered_violin_max_num_bins=27)\n",
    "\n",
    "plt.savefig(out_path + 'LighGBM_shap_summary_plot_'+ste+'_'+conti_local[ste]+'.jpg',bbox_inches='tight',pad_inches=0,dpi = 600,)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0c5b4d",
   "metadata": {},
   "source": [
    "# LightGBM prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b41fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import sklearn\n",
    "import time\n",
    "import jenkspy\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "import scipy.stats as stats\n",
    "\n",
    "metric_ste = np.zeros((6,6))\n",
    "continentList = [ \"Africa\",\"Asia\",\"Europe\", \"South America\", \"Oceania\", \"North America\"] # \n",
    "\n",
    "#continentList = [\"Africa\"]\n",
    "stei = 0\n",
    "XV = ['N11','N12','N13'\n",
    "  ,'N14','N0','N15','N16','N17','N18'\n",
    "  ,'N21','N22','N23','N24','N25','N26','N27','N28','N29','N210','N211','N212','N213','N214','N215','N216',\n",
    "    'X','Y'\n",
    "  ]\n",
    "YV = ['VDMSP'] # \n",
    "\n",
    "for ste in continentList:\n",
    "    data_name = 'Nei2'+ste\n",
    "    ### load model\n",
    "    print('load model')\n",
    "    model_ste = lgb.Booster(model_file = './Light_GDP/PSO_BPNN/LtGBM_model'+data_name+'2013_LogX.txt')\n",
    "    for year in range(2013,2023):\n",
    "        data_df = pd.read_csv('./Light_GDP/PSO_BPNN/'+data_name+str(year)+'.csv',sep = ',')\n",
    "        #checkpoint_path = \"./Light_GDP/PSO_BPNN/\"+ste+'cp.ckpt'\n",
    "        #model_ste = Create_model()\n",
    "        #model_ste.load_weights(checkpoint_path)\n",
    "        \n",
    "        XV_log = ['N11','N12','N13'\n",
    "          ,'N14','N0','N15','N16','N17','N18'\n",
    "          ,'N21','N22','N23','N24','N25','N26','N27','N28','N29','N210','N211','N212','N213','N214','N215','N216'\n",
    "          ]\n",
    "        X =  data_df.loc[:,XV]\n",
    "        X.loc[:,XV_log] = np.log(X.loc[:,XV_log]+1)\n",
    "        print('do prediction')\n",
    "        y_pred = model_ste.predict(X)\n",
    "        data_df['DMSP_like_NPP'] = y_pred \n",
    "        \n",
    "        if year == 2013:\n",
    "            stei +=1\n",
    "            y = data_df.loc[:,['VDMSP']]\n",
    "            pearson_r = stats.pearsonr((np.array(y)).reshape(-1,), (y_pred).reshape(-1,))\n",
    "            R2 = pearson_r[0]**2\n",
    "            print(R2)\n",
    "            metric_ste[stei,0] = R2\n",
    "            \n",
    "\n",
    "\n",
    "        raster_npp = rio.open(\"./Light_GDP/DMSL-NOAA/NPP/Continent/NPP_NTL_MaxFilt_SaturCotinMedian_\"+str(year)+\"Ste\"+ste+\".tif\")\n",
    "        data_npp = raster_npp.read(1)\n",
    "        data_npp[data_df.rowid,data_df.colid] = data_df.DMSP_like_NPP\n",
    "        out_transform = raster_npp.transform\n",
    "        \n",
    "\n",
    "                \n",
    "        data_npp[data_npp<0] = 0\n",
    "        \n",
    "        out_meta = raster_npp.meta.copy()\n",
    "        out_meta.update({\"driver\": \"GTiff\", \"height\": data_npp.shape[0], \"width\": data_npp.shape[1], \"transform\": out_transform})\n",
    "        print('write raster')\n",
    "        with rio.open(\"./Light_GDP/DMSL-NOAA/Pred_NPP/NeiLtGBMPredNPPMedian\"+ste+str(year)+\"tf257.tif\", \"w\", **out_meta,) as dest:\n",
    "            dest.write(data_npp,1)\n",
    "        print(str(year) + ': ' + ste)\n",
    "raster_npp.close()\n",
    "#raster_dmsp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cf58dc",
   "metadata": {},
   "source": [
    "## combine two prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ee96b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import scipy.stats as stats\n",
    "import rasterio as rio\n",
    "import rasterio\n",
    "\n",
    "continentList = [\"Africa\",\"Asia\", \"Europe\", \"South America\", \"Oceania\", \"North America\"] #\n",
    "method = 'MLP'\n",
    "\n",
    "for ste in continentList:\n",
    "    for year in range(2013,2023):\n",
    "        \n",
    "        #if str(year) in file and file.endswith('.tif'):\n",
    "        src_MLP = rasterio.open(\"./Light_GDP/DMSL-NOAA/Pred_NPP/NeiMLPPredNPPMedian\"+ste+str(year)+\"tf257.tif\",compress=\"lzw\")\n",
    "        #if src.transform[5] < 0:\n",
    "         #   reproject_raster_toflUpDown(year, block)\n",
    "          #  src = rasterio.open(datapath+'NPP_NTL_SaturCotin_'+str(year)+'_lon'+str(block)+'_rp.tif')\n",
    "        src_LtGBM = rasterio.open(\"./Light_GDP/DMSL-NOAA/Pred_NPP/NeiLtGBMPredNPPMedian\"+ste+str(year)+\"tf257.tif\",compress=\"lzw\")\n",
    "        \n",
    "        src_DMSP = rio.open(\"./Light_GDP/DMSL-NOAA/DMSP/DMSP_NTL_SaturCotin_2013Ste\"+ste+\".tif\")\n",
    "        \n",
    "        data_DMSP = src_DMSP.read()[0]\n",
    "        \n",
    "        \n",
    "        data_MLP = src_MLP.read(1)\n",
    "        data_LtGBM = src_LtGBM.read(1)\n",
    "        \n",
    "        if year == 2013:\n",
    "            \n",
    "            R2_MLP = stats.pearsonr((np.array(data_DMSP[np.logical_and(data_DMSP>0, data_MLP>0)])).reshape(-1,), \n",
    "                                (data_MLP[np.logical_and(data_DMSP>0, data_MLP>0)]).reshape(-1,))[0] ** 2\n",
    "\n",
    "            R2_LtGBM = stats.pearsonr((np.array(data_DMSP[np.logical_and(data_DMSP>0, data_LtGBM>0)])).reshape(-1,), \n",
    "                                (data_LtGBM[np.logical_and(data_DMSP>0, data_LtGBM>0)]).reshape(-1,))[0] ** 2\n",
    "        \n",
    "        data_cb = ((R2_MLP/(R2_MLP+R2_LtGBM)) * data_MLP) + ((R2_LtGBM/(R2_MLP+R2_LtGBM)) * data_LtGBM)\n",
    "        \n",
    "    \n",
    "        out_meta = src_LtGBM.meta.copy()\n",
    "\n",
    "        with rasterio.open(\"./Light_GDP/DMSL-NOAA/Pred_NPP/CbNeiPredNPPMedian\"+ste+str(year)+\"tf257.tif\", \"w\", **out_meta,) as dest:\n",
    "            dest.write(data_cb, 1)\n",
    "        print(year, ste)\n",
    "\n",
    "src_LtGBM.close()\n",
    "src_MLP.close()\n",
    "src_DMSP.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0de1df8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_DMSP , data_MLP, data_cb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83953263",
   "metadata": {},
   "source": [
    "## Mosaic as global data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e9b021d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import rasterio as rio\n",
    "import rasterio\n",
    "from rasterio.merge import merge\n",
    "from rasterio.mask import mask\n",
    "from rasterio import Affine\n",
    "import matplotlib.pyplot as plt\n",
    "from rasterio.plot import show\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import jenkspy\n",
    "import scipy\n",
    "from scipy.spatial import distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb19bb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "continentList = [\"Africa\",\"Asia\", \"Europe\", \"South America\", \"Oceania\", \"North America\"] #\n",
    "method = 'MLP'\n",
    "\n",
    "for year in range(2013,2023):\n",
    "    files_to_mosaic = []\n",
    "    for ste in continentList:\n",
    "        #if str(year) in file and file.endswith('.tif'):\n",
    "        src = rasterio.open(\"./Light_GDP/DMSL-NOAA/Pred_NPP/CbNeiPredNPPMedian\"+ste+str(year)+\"tf257.tif\",compress=\"lzw\")\n",
    "        #if src.transform[5] < 0:\n",
    "         #   reproject_raster_toflUpDown(year, block)\n",
    "          #  src = rasterio.open(datapath+'NPP_NTL_SaturCotin_'+str(year)+'_lon'+str(block)+'_rp.tif')\n",
    "        files_to_mosaic.append(src)\n",
    "        \n",
    "\n",
    "    mosaic, out_transform = merge(files_to_mosaic)\n",
    "\n",
    "    out_meta = src.meta.copy()\n",
    "    out_meta.update({\"driver\": \"GTiff\", \"height\": mosaic.shape[1], \"width\": mosaic.shape[2], \"transform\": out_transform})\n",
    "\n",
    "    with rasterio.open(\"E:/0 job/data/Light_GDP/Pred_Continue/CbNeiGlobalPredNPPMedian\"+str(year)+\"tf257.tif\", \"w\", **out_meta,) as dest:\n",
    "        dest.write(mosaic)\n",
    "\n",
    "    dest.close()\n",
    "    src.close()\n",
    "    print(str(year))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec4bbde",
   "metadata": {},
   "source": [
    "### mask outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664b77ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import scipy.stats as stats\n",
    "import rasterio as rio\n",
    "import rasterio\n",
    "\n",
    "year_sum_mean_count = np.zeros((12,3))\n",
    "continentList = [\"Africa\",\"Asia\", \"Europe\", \"South America\", \"Oceania\", \"North America\"] #\n",
    "method = 'LtGBM'\n",
    "\n",
    "src_DMSP = rio.open(\"E:/0 job/data/Light_GDP/Pred_Continue/AGlobalDMSPSaturCotin\"+str(2013)+\".tif\",compress=\"lzw\")\n",
    "\n",
    "mask_out = src_DMSP.read(1) <= 0\n",
    "\n",
    "for year in range(2013,2023):\n",
    "    \n",
    "    src_DMSP_like = rio.open(\"E:/0 job/data/Light_GDP/Pred_Continue/CbNeiGlobalPredNPPMedian\"+str(year)+\"tf257.tif\",compress=\"lzw\")\n",
    "    data_DMSP_like = src_DMSP_like.read()[0]\n",
    "    data_DMSP_like[mask_out] = 0\n",
    "    \n",
    "    year_sum_mean_count[year-2013,0] = np.nansum(data_DMSP_like)\n",
    "    year_sum_mean_count[year-2013,1] = np.nanmean(data_DMSP_like[data_DMSP_like>0])\n",
    "    year_sum_mean_count[year-2013,2] = np.count_nonzero(data_DMSP_like[data_DMSP_like>0])\n",
    "    \n",
    "    out_meta = src_DMSP_like.meta.copy()\n",
    "\n",
    "    with rasterio.open(\"E:/0 job/data/Light_GDP/Pred_Continue/CbMaskNeiGlobalPredNPPMedian\"+str(year)+\"tf257.tif\", \"w\", **out_meta,) as dest:\n",
    "        dest.write(data_DMSP_like, 1)\n",
    "    print(year)\n",
    "\n",
    "src_DMSP_like.close()\n",
    "src_DMSP.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e7a450",
   "metadata": {},
   "source": [
    "For the continuity verification of some data, although the fitting accuracy is high, the discontinuity still exists, and further continuity correction is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f9c565",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "year_sumv_DisConti = np.zeros((32,2))\n",
    "year_sumv_Conti = np.zeros((32,2))\n",
    "\n",
    "raster_pre = rio.open(\"E:/0 job/data/Light_GDP/Pred_Continue/AGlobalDMSPSaturCotin\"+str(2013)+\".tif\")\n",
    "data_pre = raster_pre.read(1)\n",
    "year_sumv_DisConti[0,0] = 2012\n",
    "year_sumv_DisConti[0,1] = np.nansum(data_pre)\n",
    "\n",
    "year_sumv_Conti[0,0] = 2012\n",
    "year_sumv_Conti[0,1] = np.nansum(data_pre)\n",
    "\n",
    "raster_cur = rio.open(\"E:/0 job/data/Light_GDP/Pred_Continue/CbMaskNeiGlobalPredNPPMedian\"+str(2013)+\"tf257.tif\")\n",
    "data_cur = raster_cur.read(1)\n",
    "year_sumv_DisConti[1,0] = 2013\n",
    "year_sumv_DisConti[1,1] = np.nansum(data_cur)\n",
    "out_meta = raster_cur.meta.copy()\n",
    "\n",
    "for year in range(2013,2022):\n",
    "    \n",
    "    raster_aft = rio.open(\"E:/0 job/data/Light_GDP/Pred_Continue/CbMaskNeiGlobalPredNPPMedian\"+str(year+1)+\"tf257.tif\")\n",
    "    print(year)\n",
    "    data_aft = raster_aft.read(1)\n",
    "    year_sumv_DisConti[year+1-2012,0] = year+1\n",
    "    year_sumv_DisConti[year+1-2012,1] = np.nansum(data_aft)\n",
    "    \n",
    "    data_logi = np.logical_and(data_cur<data_pre, data_aft!=0)\n",
    "\n",
    "    data_cur[data_logi] = data_pre[data_logi]\n",
    "    year_sumv_Conti[year-2012,0] = year\n",
    "    year_sumv_Conti[year-2012,1] = np.nansum(data_cur)\n",
    "    \n",
    "    with rio.open(\"E:/0 job/data/Light_GDP/Pred_Continue/CtCbMaskNeiGlobalPredNPPMedian\"+str(year)+\"tf257.tif\", \"w\", **out_meta,) as dest:\n",
    "        dest.write(data_cur,1)\n",
    "    print(year)\n",
    "    data_pre = data_cur\n",
    "    data_cur = data_aft\n",
    "\n",
    "year = year +1\n",
    "data_logi = data_cur<data_pre\n",
    "data_cur[data_logi] = data_pre[data_logi]\n",
    "\n",
    "year_sumv_Conti[year-2012,0] = year\n",
    "year_sumv_Conti[year-2012,1] = np.nansum(data_cur)\n",
    "\n",
    "with rio.open(\"E:/0 job/data/Light_GDP/Pred_Continue/CtCbMaskNeiGlobalPredNPPMedian\"+str(year)+\"tf257.tif\", \"w\", **out_meta,) as dest:\n",
    "    dest.write(data_cur,1)\n",
    "  \n",
    "        \n",
    "\n",
    "print(year) \n",
    "raster_pre.close()\n",
    "raster_cur.close()\n",
    "raster_aft.close()\n",
    "del data_pre,data_cur,data_aft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "id": "259224b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "raster_pre.close()\n",
    "raster_cur.close()\n",
    "raster_aft.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ec4d2e88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGsCAYAAAAPJKchAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAogUlEQVR4nO3dfXRU9YH/8c8AyWRiZiYtQyDRKMGmoilZUugKPkQpFijIoWuW3aIrCOqRxRYtiwtxaWm1luB6LAfaXcpWUyG72t0NurSwIFRI1EYbNAHrCmQ1EpuH4qjMJMZkEnJ/f/SXWWMeJ8nMNzN5v86ZP+Y+TL73ezzM27n3ztgsy7IEAABgyBjTAwAAAKMbMQIAAIwiRgAAgFHECAAAMIoYAQAARhEjAADAKGIEAAAYRYwAAACjiBEAAGAUMQIAAIyKqhgpLS3V4sWLlZaWJpvNpueeey7k1zh06JBmzZolp9OpCRMmKC8vT9XV1cM/WAAAMCBRFSMff/yx/uzP/kw/+clPBrX/O++8oyVLluirX/2qKisrdejQIXm9Xt1yyy3DPFIAADBQtmj9oTybzaZnn31W3/jGN4LLAoGANm3apH/913/V+fPn9aUvfUlbt27VjTfeKEn6z//8Ty1btkytra0aM+ZPHfarX/1KS5YsUWtrq+Li4gwcCQAAo1tUfTLSn5UrV+rll1/WM888o5MnT2rp0qVasGCBqqqqJEkzZ87U2LFjVVhYqAsXLsjn82nPnj2aN28eIQIAgCEx88nI22+/rczMTP3hD39QWlpacLubbrpJf/7nf64f/ehHkv503cnSpUv1wQcf6MKFC5o9e7YOHDig5ORkA0cBAABi5pOR119/XZZl6Ytf/KKSkpKCj5KSEr399tuSpIaGBt11111asWKFysvLVVJSovj4eP3lX/6lorTJAACIeuNMD2C4dHR0aOzYsXrttdc0duzYLuuSkpIkST/96U/lcrn06KOPBtcVFRUpPT1dr776qmbNmhXRMQMAgBiKkZycHF24cEHnzp3T9ddf3+M2zc3N3UKl83lHR0fYxwgAALqLqtM0TU1NqqysVGVlpSSpurpalZWVqqmp0Re/+EXddtttWr58ufbu3avq6mqVl5dr69atOnDggCRp0aJFKi8v10MPPaSqqiq9/vrrWrlypS677DLl5OQYPDIAAEavqLqA9dixY5ozZ0635StWrNAvfvELtbW16Yc//KF2796t2tpajR8/XrNnz9YPfvADTZs2TZL0zDPP6NFHH9WZM2eUmJio2bNna+vWrZo6dWqkDwcAACjKYgQAAMSeqDpNAwAAYg8xAgAAjIqKu2k6OjpUV1cnp9Mpm81mejgAAGAALMtSY2Oj0tLSgj/D0pOoiJG6ujqlp6ebHgYAABiE9957T5dcckmv66MiRpxOp6Q/HYzL5TI8GgAAMBB+v1/p6enB9/HeREWMdJ6acblcxAgAAFGmv0ssuIAVAAAYRYwAAACjiBEAAGAUMQIAAIwiRgAAgFHECAAAMIoYAQAARhEjAADAKGIEAAAYFRXfwAoAAIafrzkgb1NA/pY2uRxx8lwUL3difMTHQYwAADAK1Z3/RBuKT+rFKm9wWW6mRwV52UpLdkR0LJymAQBglPE1B7qFiCSVVnm1sfikfM2BiI6HGAEAYJTxNgW6hUin0iqvvE3ECAAACCN/S1uf6xv7WT/ciBEAAEYZV0Jcn+ud/awfbsQIAACjjCcpXrmZnh7X5WZ65EmK7B01xAgAAKOMOzFeBXnZ3YIkN9OjrXnZEb+9l1t7AQAYhdKSHdqxLEfepoAaW9rkTIiTJ4nvGQEAABHkTjQTH5/FaRoAAGAUMQIAAIwKOUYaGxt1//3367LLLpPD4dA111yj8vLyPvcpKSnRjBkzlJCQoClTpmjnzp2DHjAAAIgtIcfIXXfdpcOHD2vPnj164403NG/ePN10002qra3tcfvq6motXLhQ119/vSoqKvTggw9q7dq1Ki4uHvLgAQBA9LNZlmUNdONPPvlETqdT//Vf/6VFixYFl0+fPl0333yzfvjDH3bbZ8OGDdq3b5/eeuut4LLVq1frxIkTKisrG9Df9fv9crvd8vl8crlcAx0uAAAwaKDv3yF9MtLe3q4LFy4oISGhy3KHw6GXXnqpx33Kyso0b968Lsvmz5+v48ePq62t56+bbW1tld/v7/IAAACxKaQYcTqdmj17th5++GHV1dXpwoULKioq0quvvqr6+voe92loaNDEiRO7LJs4caLa29vl9fb8Iz1btmyR2+0OPtLT00MZJgAAiCIhXzOyZ88eWZaliy++WHa7Xdu3b9ett96qsWPH9rqPzWbr8rzzzNBnl3fKz8+Xz+cLPt57771QhwkAAKJEyF96dvnll6ukpEQff/yx/H6/UlNT9dd//dfKyMjocftJkyapoaGhy7Jz585p3LhxGj9+fI/72O122e32UIcGAACi0KC/Z+Siiy5SamqqPvroIx06dEhLlizpcbvZs2fr8OHDXZY9//zzmjlzpuLiIvurgAAAYOQJOUYOHTqkgwcPqrq6WocPH9acOXN0xRVXaOXKlZL+dIpl+fLlwe1Xr16ts2fPat26dXrrrbf05JNP6oknntD69euH7ygAAEDUCjlGfD6f7r33Xk2dOlXLly/Xddddp+effz74KUd9fb1qamqC22dkZOjAgQM6duyYpk+frocffljbt29XXl7e8B0FAACIWiF9z4gpfM8IAADRJyzfMwIAADDciBEAAGAUMQIAAIwiRgAAgFHECAAAMIoYAQAARhEjAADAKGIEAAAYRYwAAACjiBEAAGAUMQIAAIwiRgAAgFHECAAAMIoYAQAARhEjAADAKGIEAAAYRYwAAACjiBEAAGAUMQIAAIwiRgAAgFHECAAAMIoYAQAARhEjAADAKGIEAAAYRYwAAACjiBEAAGDUONMDAAAgFviaA/I2BeRvaZPLESfPRfFyJ8abHlZUIEYAABiiuvOfaEPxSb1Y5Q0uy830qCAvW2nJDoMjiw6cpgEAYAh8zYFuISJJpVVebSw+KV9zwNDIogcxAgDAEHibAt1CpFNplVfeJmKkP8QIAABD4G9p63N9Yz/rQYwAADAkroS4Ptc7+1kPYgQAgCHxJMUrN9PT47rcTI88SdxR0x9iBACAIXAnxqsgL7tbkORmerQ1L5vbeweAW3sBABiitGSHdizLkbcpoMaWNjkT4uRJ4ntGBooYAQBgGLgTiY/B4jQNAAAwihgBAABGESMAAMAoYgQAABhFjAAAAKOIEQAAYBQxAgAAjCJGAACAUcQIAAAwihgBAABGESMAAMAoYgQAABhFjAAAAKOIEQAAYBQxAgAAjCJGAACAUcQIAAAwihgBAABGESMAAMCokGKkvb1dmzZtUkZGhhwOh6ZMmaKHHnpIHR0dve5z7Ngx2Wy2bo9Tp04NefAAACD6jQtl461bt2rnzp166qmnlJWVpePHj2vlypVyu9267777+tz39OnTcrlcwecTJkwY3IgBAEBMCSlGysrKtGTJEi1atEiSNHnyZD399NM6fvx4v/umpKQoOTl5UIMEAACxK6TTNNddd51+85vf6MyZM5KkEydO6KWXXtLChQv73TcnJ0epqamaO3eujh492ue2ra2t8vv9XR4AACA2hfTJyIYNG+Tz+TR16lSNHTtWFy5c0COPPKJly5b1uk9qaqp27dqlGTNmqLW1VXv27NHcuXN17Ngx5ebm9rjPli1b9IMf/CC0IwEAAFHJZlmWNdCNn3nmGT3wwAP6x3/8R2VlZamyslL333+/Hn/8ca1YsWLAf3Tx4sWy2Wzat29fj+tbW1vV2toafO73+5Weni6fz9fluhMAADBy+f1+ud3uft+/Q/pk5IEHHtDGjRv1zW9+U5I0bdo0nT17Vlu2bAkpRmbNmqWioqJe19vtdtnt9lCGBgAAolRI14w0NzdrzJiuu4wdO7bPW3t7UlFRodTU1JD2AQAAsSmkT0YWL16sRx55RJdeeqmysrJUUVGhxx9/XKtWrQpuk5+fr9raWu3evVuStG3bNk2ePFlZWVkKBAIqKipScXGxiouLh/dIAAAxw9cckLcpIH9Lm1yOOHkuipc7Md70sBAmIcXIjh079N3vfldr1qzRuXPnlJaWpnvuuUff+973gtvU19erpqYm+DwQCGj9+vWqra2Vw+FQVlaW9u/fP6A7cAAAo0/d+U+0ofikXqzyBpflZnpUkJettGSHwZEhXEK6gNWUgV4AAwCIbr7mgL71dEWXEOmUm+nRjmU5fEISRQb6/s1v0wAARgxvU6DHEJGk0iqvvE2BCI8IkUCMAABGDH9LW5/rG/tZj+hEjAAARgxXQlyf6539rEd0IkYAACOGJyleuZmeHtflZnrkSeJ6kVhEjAAARgx3YrwK8rK7BUlupkdb87K5eDVGhXRrLwAA4ZaW7NCOZTnyNgXU2NImZ0KcPEl8z0gsI0YAACOOO5H4GE04TQMAAIwiRgAAgFHECAAAMIoYAQAARhEjAADAKGIEAAAYRYwAAACjiBEAAGAUX3oGADHO1xyQtykgf0ubXI44eS7iC8UwshAjABDD6s5/og3FJ/VilTe4LDfTo4K8bKUlOwyODPg/nKYBgBjlaw50CxFJKq3yamPxSfmaA4ZGBnRFjABAjPI2BbqFSKfSKq+8TcQIRgZiBABilL+lrc/1jf2sByKFGAGAGOVKiOtzvbOf9UCkECMAEKM8SfHKzfT0uC430yNPEnfUYGQgRgAgRrkT41WQl90tSHIzPdqal83tvRgxuLUXAGJYWrJDO5blyNsUUGNLm5wJcfIk8T0jGFmIEQCIce5E4gMjG6dpAACAUcQIAAAwihgBAABGESMAAMAoYgQAABhFjAAAAKOIEQAAYBQxAgAAjCJGAACAUcQIAAAwihgBAABGESMAAMAoYgQAABhFjAAAAKOIEQAAYBQxAgAAjCJGAACAUcQIAAAwihgBAABGESMAAMAoYgQAABhFjAAAAKOIEQAAYBQxAgAAjCJGAACAUcQIAAAwihgBAABGjTM9AACIJr7mgLxNAflb2uRyxMlzUbzcifGmhwVENWIEAAao7vwn2lB8Ui9WeYPLcjM9KsjLVlqyw+DIgOjGaRoAGABfc6BbiEhSaZVXG4tPytccMDQyIPqFFCPt7e3atGmTMjIy5HA4NGXKFD300EPq6Ojoc7+SkhLNmDFDCQkJmjJlinbu3DmkQQNApHmbAt1CpFNplVfeJmIEGKyQTtNs3bpVO3fu1FNPPaWsrCwdP35cK1eulNvt1n333dfjPtXV1Vq4cKHuvvtuFRUV6eWXX9aaNWs0YcIE5eXlDctBAEC4+Vva+lzf2M96AL0LKUbKysq0ZMkSLVq0SJI0efJkPf300zp+/Hiv++zcuVOXXnqptm3bJkm68sordfz4cT322GPECICo4UqI63O9s5/1AHoX0mma6667Tr/5zW905swZSdKJEyf00ksvaeHChb3uU1ZWpnnz5nVZNn/+fB0/flxtbT3/n0Rra6v8fn+XBwCY5EmKV26mp8d1uZkeeZK4owYYrJBiZMOGDVq2bJmmTp2quLg45eTk6P7779eyZct63aehoUETJ07ssmzixIlqb2+X19vz+dctW7bI7XYHH+np6aEMEwCGnTsxXgV52d2CJDfTo6152dzeCwxBSKdpfvnLX6qoqEj/9m//pqysLFVWVur+++9XWlqaVqxY0et+Nputy3PLsnpc3ik/P1/r1q0LPvf7/QQJAOPSkh3asSxH3qaAGlva5EyIkyeJ7xkBhiqkGHnggQe0ceNGffOb35QkTZs2TWfPntWWLVt6jZFJkyapoaGhy7Jz585p3LhxGj9+fI/72O122e32UIYGABHhTiQ+gOEW0mma5uZmjRnTdZexY8f2eWvv7Nmzdfjw4S7Lnn/+ec2cOVNxcVzwBQDAaBdSjCxevFiPPPKI9u/fr3fffVfPPvusHn/8cf3FX/xFcJv8/HwtX748+Hz16tU6e/as1q1bp7feektPPvmknnjiCa1fv374jgIAAEStkE7T7NixQ9/97ne1Zs0anTt3Tmlpabrnnnv0ve99L7hNfX29ampqgs8zMjJ04MABfec739FPf/pTpaWlafv27dzWCwAAJEk2q/Nq0hHM7/fL7XbL5/PJ5XKZHg4AABiAgb5/89s0AADAKGIEAAAYRYwAAACjiBEAAGAUMQIAAIwiRgAAgFHECAAAMIoYAQAARhEjAADAKGIEAAAYRYwAAACjiBEAAGBUSL/aCwDDydcckLcpIH9Lm1yOOHkuipc7Md70sABEGDECwIi6859oQ/FJvVjlDS7LzfSoIC9backOgyMDEGmcpgEQcb7mQLcQkaTSKq82Fp+UrzlgaGQATCBGAESctynQLUQ6lVZ55W0iRoDRhBgBEHH+lrY+1zf2sx5AbCFGAEScKyGuz/XOftYDiC3ECICI8yTFKzfT0+O63EyPPEncUQOMJsQIgIhzJ8arIC+7W5DkZnq0NS+b23uBUYZbewEYkZbs0I5lOfI2BdTY0iZnQpw8SXzPCDAaESMAjHEnEh8AOE0DAAAMI0YAAIBRxAgAADCKGAEAAEYRIwAAwChiBAAAGEWMAAAAo4gRAABgFDECAACMIkYAAIBRxAgAADCKGAEAAEYRIwAAwChiBAAAGEWMAAAAo4gRAABgFDECAACMIkYAAIBRxAgAADCKGAEAAEYRIwAAwChiBAAAGEWMAAAAo4gRAABgFDECAACMIkYAAIBRxAgAADCKGAEAAEYRIwAAwChiBAAAGEWMAAAAo4gRAABgVEgxMnnyZNlstm6Pe++9t8ftjx071uP2p06dGpbBAwCA6DculI3Ly8t14cKF4PPf//73+trXvqalS5f2ud/p06flcrmCzydMmBDiMAEAQKwKKUY+GxEFBQW6/PLLdcMNN/S5X0pKipKTk0MeHAAAiH2DvmYkEAioqKhIq1atks1m63PbnJwcpaamau7cuTp69Gi/r93a2iq/39/lAQAAYtOgY+S5557T+fPndccdd/S6TWpqqnbt2qXi4mLt3btXV1xxhebOnavS0tI+X3vLli1yu93BR3p6+mCHCQAARjibZVnWYHacP3++4uPj9atf/Sqk/RYvXiybzaZ9+/b1uk1ra6taW1uDz/1+v9LT0+Xz+bpcewIAAEYuv98vt9vd7/t3SNeMdDp79qyOHDmivXv3hrzvrFmzVFRU1Oc2drtddrt9MEMDAABRZlCnaQoLC5WSkqJFixaFvG9FRYVSU1MH82cBAEAMCvmTkY6ODhUWFmrFihUaN67r7vn5+aqtrdXu3bslSdu2bdPkyZOVlZUVvOC1uLhYxcXFwzN6AAAQ9UKOkSNHjqimpkarVq3qtq6+vl41NTXB54FAQOvXr1dtba0cDoeysrK0f/9+LVy4cGijBgAAMWPQF7BG0kAvgAEAACPHQN+/+W0aAABg1KDupgEw8viaA/I2BeRvaZPLESfPRfFyJ8abHhYA9IsYAWJA3flPtKH4pF6s8gaX5WZ6VJCXrbRkh8GRAUD/OE0DRDlfc6BbiEhSaZVXG4tPytccMDQyABgYYgSIct6mQLcQ6VRa5ZW3iRgBMLIRI0CU87e09bm+sZ/1AGAaMQJEOVdCXJ/rnf2sBwDTiBEgynmS4pWb6elxXW6mR54k7qgBMLIRI0CUcyfGqyAvu1uQ5GZ6tDUvm9t7AYx43NoLxIC0ZId2LMuRtymgxpY2ORPi5Enie0YARAdiBIgR7kTiA0B04jQNAAAwihgBAABGESMAAMAoYgQAABhFjAAAAKOIEQAAYBQxAgAAjCJGAACAUcQIAAAwihgBAABGESMAAMAoYgQAABhFjAAAAKOIEQAAYBQxAgAAjCJGAACAUeNMDwAYiXzNAXmbAvK3tMnliJPnoni5E+NNDwsAYhIxAnxG3flPtKH4pF6s8gaX5WZ6VJCXrbRkh8GRAUBs4jQN8Cm+5kC3EJGk0iqvNhaflK85YGhkABC7iBHgU7xNgW4h0qm0yitvEzECAMONGAE+xd/S1uf6xn7WAwBCR4wAn+JKiOtzvbOf9QCA0BEjwKd4kuKVm+npcV1upkeeJO6oAYDhRowAn+JOjFdBXna3IMnN9GhrXja39wJAGHBrL/AZackO7ViWI29TQI0tbXImxMmTxPeMAEC4ECNAD9yJxAcARAoxgrDj20wBAH0hRhBWfJspAKA/XMCKsOHbTAEAA0GMIGz4NlMAwEAQIwgbvs0UADAQxAjChm8zBQAMBDGCsOHbTAEAA0GMIGz4NlMAwEBway/Cim8zBQD0hxhB2PFtpgCAvnCaBgAAGEWMAAAAo4gRAABgFDECAACMIkYAAIBRxAgAADCKGAEAAEYRIwAAwKiQYmTy5Mmy2WzdHvfee2+v+5SUlGjGjBlKSEjQlClTtHPnziEPGgAAxI6QYqS8vFz19fXBx+HDhyVJS5cu7XH76upqLVy4UNdff70qKir04IMPau3atSouLh76yAEAQEwI6evgJ0yY0OV5QUGBLr/8ct1www09br9z505deuml2rZtmyTpyiuv1PHjx/XYY48pLy9vcCMGAAAxZdDXjAQCARUVFWnVqlWy2Ww9blNWVqZ58+Z1WTZ//nwdP35cbW1tvb52a2ur/H5/lwcAAIhNg46R5557TufPn9cdd9zR6zYNDQ2aOHFil2UTJ05Ue3u7vF5vr/tt2bJFbrc7+EhPTx/sMAEAwAg36Bh54okn9PWvf11paWl9bvfZT00sy+px+afl5+fL5/MFH++9995ghwkAAEa4kK4Z6XT27FkdOXJEe/fu7XO7SZMmqaGhocuyc+fOady4cRo/fnyv+9ntdtnt9sEMDQAARJlBfTJSWFiolJQULVq0qM/tZs+eHbzjptPzzz+vmTNnKi4ubjB/GgAAxJiQY6Sjo0OFhYVasWKFxo3r+sFKfn6+li9fHny+evVqnT17VuvWrdNbb72lJ598Uk888YTWr18/9JEDAICYEHKMHDlyRDU1NVq1alW3dfX19aqpqQk+z8jI0IEDB3Ts2DFNnz5dDz/8sLZv385tvQAAIMhmdV5ROoL5/X653W75fD65XC7TwwEAAAMw0PdvfpsGAAAYRYwAAACjBnVrL8zwNQfkbQrI39ImlyNOnovi5U6MNz0sAACGhBiJEnXnP9GG4pN6ser/vrk2N9OjgrxspSU7DI4MAICh4TRNFPA1B7qFiCSVVnm1sfikfM0BQyMDAGDoiJEo4G0KdAuRTqVVXnmbiBEAQPQiRqKAv6X3XziWpMZ+1gMAMJIRI1HAldD3V+c7+1kPAMBIRoxEAU9SvHIzPT2uy830yJPEHTUAgOhFjEQBd2K8CvKyuwVJbqZHW/Oyub0XABDVuLU3SqQlO7RjWY68TQE1trTJmRAnTxLfMwIAiH7ESBRxJxIfAIDYw2kaAABgFDECAACMIkYAAIBRxAgAADCKGAEAAEYRIwAAwChiBAAAGEWMAAAAo4gRAABgFDECAACMIkYAAIBRxAgAADCKGAEAAEYRIwAAwChiBAAAGEWMAAAAo4gRAABgFDECAACMIkYAAIBRxAgAADCKGAEAAEYRIwAAwChiBAAAGEWMAAAAo4gRAABgFDECAACMIkYAAIBRxAgAADCKGAEAAEYRIwAAwChiBAAAGEWMAAAAo4gRAABgFDECAACMIkYAAIBRxAgAADCKGAEAAEYRIwAAwKhxpgdgiq85IG9TQP6WNrkccfJcFC93YrzpYQEAMOqMyhipO/+JNhSf1ItV3uCy3EyPCvKylZbsMDgyAABGn1F3msbXHOgWIpJUWuXVxuKT8jUHDI0MAIDRadTFiLcp0C1EOpVWeeVtIkYAAIikURcj/pa2Ptc39rMeAAAMr5BjpLa2Vn/zN3+j8ePHKzExUdOnT9drr73W6/bHjh2TzWbr9jh16tSQBj5YroS4Ptc7+1kPAACGV0gXsH700Ue69tprNWfOHP33f/+3UlJS9Pbbbys5ObnffU+fPi2XyxV8PmHChJAHOxw8SfHKzfSotIdTNbmZHnmSuKMGAIBICilGtm7dqvT0dBUWFgaXTZ48eUD7pqSkDChaws2dGK+CvGxtLD7ZJUhyMz3ampfN7b0AAERYSDGyb98+zZ8/X0uXLlVJSYkuvvhirVmzRnfffXe/++bk5KilpUVXXXWVNm3apDlz5vS6bWtrq1pbW4PP/X5/KMPsV1qyQzuW5cjbFFBjS5ucCXHyJPE9IwAAmBDSNSPvvPOO/vmf/1mZmZk6dOiQVq9erbVr12r37t297pOamqpdu3apuLhYe/fu1RVXXKG5c+eqtLS01322bNkit9sdfKSnp4cyzAFxJ8br8pQkTb/0c7o8JYkQAQDAEJtlWdZAN46Pj9fMmTP129/+Nrhs7dq1Ki8vV1lZ2YD/6OLFi2Wz2bRv374e1/f0yUh6erp8Pl+X604AAMDI5ff75Xa7+33/DumTkdTUVF111VVdll155ZWqqakJaXCzZs1SVVVVr+vtdrtcLleXBwAAiE0hxci1116r06dPd1l25swZXXbZZSH90YqKCqWmpoa0DwAAiE0hXcD6ne98R9dcc41+9KMf6a/+6q/0u9/9Trt27dKuXbuC2+Tn56u2tjZ4Hcm2bds0efJkZWVlKRAIqKioSMXFxSouLh7eIwEAAFEppBj5yle+omeffVb5+fl66KGHlJGRoW3btum2224LblNfX9/ltE0gEND69etVW1srh8OhrKws7d+/XwsXLhy+owAAAFErpAtYTRnoBTAAAGDkCMsFrAAAAMONGAEAAEYRIwAAwChiBAAAGBXS3TSmdF5jO9y/UQMAAMKn8327v3tloiJGGhsbJSksv1EDAADCq7GxUW63u9f1UXFrb0dHh+rq6uR0OmWz2YbtdTt/8+a9997jluEwY64jg3mODOY5MpjnyAjnPFuWpcbGRqWlpWnMmN6vDImKT0bGjBmjSy65JGyvz+/fRA5zHRnMc2Qwz5HBPEdGuOa5r09EOnEBKwAAMIoYAQAARo3qGLHb7dq8ebPsdrvpocQ85joymOfIYJ4jg3mOjJEwz1FxASsAAIhdo/qTEQAAYB4xAgAAjCJGAACAUcQIAAAwKupjZMuWLfrKV74ip9OplJQUfeMb39Dp06e7bGNZlr7//e8rLS1NDodDN954o958880u2+zatUs33nijXC6XbDabzp8/32X9u+++qzvvvFMZGRlyOBy6/PLLtXnzZgUCgXAf4ogQqXn+tNbWVk2fPl02m02VlZVhOKqRJ9LzvH//fl199dVyOBzyeDy65ZZbwnVoI0ok5/nMmTNasmSJPB6PXC6Xrr32Wh09ejSchzdiDMc8f/jhh/r2t7+tK664QomJibr00ku1du1a+Xy+Lq/z0Ucf6fbbb5fb7Zbb7dbtt9/e578vsSZScx2u98Koj5GSkhLde++9euWVV3T48GG1t7dr3rx5+vjjj4PbPProo3r88cf1k5/8ROXl5Zo0aZK+9rWvBX/zRpKam5u1YMECPfjggz3+nVOnTqmjo0M/+9nP9Oabb+rHP/6xdu7c2ev2sSZS8/xpf//3f6+0tLSwHM9IFcl5Li4u1u23366VK1fqxIkTevnll3XrrbeG9fhGikjO86JFi9Te3q4XXnhBr732mqZPn66bb75ZDQ0NYT3GkWA45rmurk51dXV67LHH9MYbb+gXv/iFDh48qDvvvLPL37r11ltVWVmpgwcP6uDBg6qsrNTtt98e0eM1KVJzHbb3QivGnDt3zpJklZSUWJZlWR0dHdakSZOsgoKC4DYtLS2W2+22du7c2W3/o0ePWpKsjz76qN+/9eijj1oZGRnDNvZoEu55PnDggDV16lTrzTfftCRZFRUV4TiMES9c89zW1mZdfPHF1s9//vOwjj9ahGue33//fUuSVVpaGlzm9/stSdaRI0fCczAj2FDnudO///u/W/Hx8VZbW5tlWZb1P//zP5Yk65VXXgluU1ZWZkmyTp06FaajGdnCNdc9GY73wqj/ZOSzOj9O+vznPy9Jqq6uVkNDg+bNmxfcxm6364YbbtBvf/vbIf+tzr8z2oRznv/4xz/q7rvv1p49e5SYmDh8g45C4Zrn119/XbW1tRozZoxycnKUmpqqr3/9691OQ4wW4Zrn8ePH68orr9Tu3bv18ccfq729XT/72c80ceJEzZgxY3gPIgoM1zz7fD65XC6NG/enn1crKyuT2+3W1VdfHdxm1qxZcrvdQ/53PlqFa65722ao74UxFSOWZWndunW67rrr9KUvfUmSgh+FTpw4scu2EydOHNLHpG+//bZ27Nih1atXD37AUSqc82xZlu644w6tXr1aM2fOHL5BR6FwzvM777wjSfr+97+vTZs26de//rU+97nP6YYbbtCHH344TEcQHcI5zzabTYcPH1ZFRYWcTqcSEhL04x//WAcPHlRycvKwHUM0GK55/uCDD/Twww/rnnvuCS5raGhQSkpKt21TUlJGxemwzwrnXH/WcL0XRsWv9g7Ut771LZ08eVIvvfRSt3U2m63Lc8uyui0bqLq6Oi1YsEBLly7VXXfdNajXiGbhnOcdO3bI7/crPz9/yOOMduGc546ODknSP/zDPygvL0+SVFhYqEsuuUT/8R//0ec/PrEmnPNsWZbWrFmjlJQUvfjii3I4HPr5z3+um2++WeXl5UpNTR3y+KPFcMyz3+/XokWLdNVVV2nz5s19vkZfrxPrwj3XnYbzvTBmPhn59re/rX379uno0aO65JJLgssnTZokSd3K79y5c90KcSDq6uo0Z84czZ49W7t27RraoKNQuOf5hRde0CuvvCK73a5x48bpC1/4giRp5syZWrFixTAcQXQI9zx3vgleddVVwWV2u11TpkxRTU3NUIYeVSLx3/Ovf/1rPfPMM7r22mv15S9/Wf/0T/8kh8Ohp556angOIgoMxzw3NjZqwYIFSkpK0rPPPqu4uLgur/PHP/6x2999//33B/XvfDQL91x3Gu73wqiPEcuy9K1vfUt79+7VCy+8oIyMjC7rMzIyNGnSJB0+fDi4LBAIqKSkRNdcc01If6u2tlY33nijvvzlL6uwsFBjxkT99A1YpOZ5+/btOnHihCorK1VZWakDBw5Ikn75y1/qkUceGZ6DGcEiNc8zZsyQ3W7vcutfW1ub3n33XV122WVDP5ARLlLz3NzcLEnd/q0YM2ZM8NOpWDZc8+z3+zVv3jzFx8dr3759SkhI6PI6s2fPls/n0+9+97vgsldffVU+ny/kf+ejVaTmWgrTe+GQLn8dAf72b//Wcrvd1rFjx6z6+vrgo7m5ObhNQUGB5Xa7rb1791pvvPGGtWzZMis1NdXy+/3Bberr662KigrrX/7lX4JXv1dUVFgffPCBZVmWVVtba33hC1+wvvrVr1p/+MMfuvyt0SBS8/xZ1dXVo+pumkjO83333WddfPHF1qFDh6xTp05Zd955p5WSkmJ9+OGHET1mEyI1z++//741fvx465ZbbrEqKyut06dPW+vXr7fi4uKsysrKiB93pA3HPPv9fuvqq6+2pk2bZv3v//5vl9dpb28Pvs6CBQus7Oxsq6yszCorK7OmTZtm3XzzzRE/ZlMiNdfhei+M+hiR1OOjsLAwuE1HR4e1efNma9KkSZbdbrdyc3OtN954o8vrbN68uc/XKSws7PVvjQaRmufPGm0xEsl5DgQC1t/93d9ZKSkpltPptG666Sbr97//fYSO1KxIznN5ebk1b9486/Of/7zldDqtWbNmWQcOHIjQkZo1HPPcedt0T4/q6urgdh988IF12223WU6n03I6ndZtt902oK9oiBWRmutwvRfa/v9BAAAAGDF6LnoAAAAjEjECAACMIkYAAIBRxAgAADCKGAEAAEYRIwAAwChiBAAAGEWMAAAAo4gRAABgFDECAACMIkYAAIBRxAgAADDq/wGBBheI2nBovwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.scatterplot(x = year_sumv_Conti[year_sumv_Conti[:,0]>0,][:,0], y = year_sumv_Conti[year_sumv_Conti[:,0]>0,][:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9171626",
   "metadata": {},
   "source": [
    "# Light to GDP：Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d3f02c",
   "metadata": {},
   "source": [
    "Focus on nested structures and spatial relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcd732c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "wksp = r''\n",
    "os.chdir(wksp)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ipyleaflet import Map, Marker\n",
    "import geopandas as gpd\n",
    "import rasterio as rio\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import convolve2d\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "sim_N = 100\n",
    "metrics_mat = np.zeros((sim_N,4))\n",
    "data_df = data_LightGDP.drop(columns='Unnamed: 0')\n",
    "data_df['year_str'] = data_df['year'].copy()\n",
    "data_df.loc[data_LightGDP.year == '2013b','year'] = 2013\n",
    "data_df.year = data_df.year.astype('int')\n",
    "\n",
    "\n",
    "# set UID and dropna\n",
    "GID_act = 'GID_1'\n",
    "XV = ['lCPGDP','lCPGDP_Cls', 'laglNTL_sum','lNTL_sum']#,'lCGDP','laglNTL_sum','lNTL_sum''year',\n",
    "YV = ['lGDP']\n",
    "\n",
    "#XV = ['lNTL_sum_diff','lCPGDP_Cls']#,'lCGDP','laglNTL_sum','lNTL_sum''year',\n",
    "#YV = ['lGDPdiff']\n",
    "data_df = data_df.dropna(subset = XV)\n",
    "\n",
    "\n",
    "data_df.index = np.arange(0,data_df.shape[0])\n",
    "\n",
    "pred_df = data_df[['GID_1', 'year_str']]\n",
    "\n",
    "for simi in range(sim_N):\n",
    "\n",
    "    #data_LightGDP = pd.read_csv(r'E:\\0 job\\data\\Light_GDP\\Pred_Continue\\GDP/Light_GDPContiNPP_train.csv',sep = ',')\n",
    "    # coby year\n",
    "\n",
    "    data_df = data_LightGDP.drop(columns='Unnamed: 0')\n",
    "    data_df['year_str'] = data_df['year']\n",
    "    data_df.loc[data_LightGDP.year == '2013b','year'] = 2013\n",
    "    data_df.year = data_df.year.astype('int')\n",
    "    # set UID and dropna\n",
    "    #XV = ['year','lCPGDP','lNTL_mean']#,'lCGDP','lCPGDP_Cls''laglNTL_sum','lNTL_sum'\n",
    "    #YV = ['lGDP']\n",
    "    data_df = data_df.dropna(subset = XV)\n",
    "    data_df.index = np.arange(0,data_df.shape[0])\n",
    "\n",
    "    GIDs = data_df[GID_act].unique()\n",
    "    data_df['GID_act_copy'] = data_df[GID_act]\n",
    "\n",
    "    if True:## one hot encoding\n",
    "        data_df = pd.get_dummies(data_df,columns=['GID_act_copy'])\n",
    "        #data_df = pd.get_dummies(data_df,columns=['year'])\n",
    "        ohe_name = np.char.add(np.repeat('GID_act_copy'+'_',len(GIDs)).astype(np.str_),np.array(GIDs).astype(np.str_))\n",
    "        #ohe_name_year = np.char.add(np.repeat('year'+'_',len(np.arange(1992,2022))).astype(np.str_), np.arange(1992,2022).astype(np.str_))\n",
    "        #XV = ['year','lCPGDP','lCPGDP_Cls', 'laglNTL_sum','lNTL_sum']#,'lCGDP','lCPGDP_Cls''laglNTL_sum','lNTL_sum'\n",
    "        X_var = np.hstack((XV, ohe_name))#ohe_name_year\n",
    "    else:\n",
    "        X_var = XV\n",
    "        \n",
    "\n",
    "    vi = np.arange(0,data_df.shape[0])\n",
    "    pred_x = data_df.loc[np.int32(vi),X_var]\n",
    "    # dropna for y\n",
    "    data_df = data_df.dropna(subset = YV)\n",
    "    data_df.index = np.arange(0,data_df.shape[0])\n",
    "\n",
    "\n",
    "    tf.random.set_seed(simi)\n",
    "\n",
    "    data_len = data_df.shape[0]\n",
    "\n",
    "    scaler_x = MinMaxScaler()\n",
    "    scaler_y = MinMaxScaler()\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "    vi_train = np.array([])\n",
    "    vi_test = np.array([])\n",
    "    vi_vali = np.array([])\n",
    "    vi = data_df.index\n",
    "    for bki in data_df.GID.unique():\n",
    "        vi_inbk = vi[data_df.GID == str(bki)]\n",
    "        vi_inbk_train, vi_inbk_test = train_test_split(vi_inbk, test_size=0.3,random_state=simi)\n",
    "        vi_inbk_train, vi_inbk_vali = train_test_split(vi_inbk_train, test_size=0.3,random_state=simi)\n",
    "        vi_train = np.append(vi_train, vi_inbk_train)\n",
    "        vi_test = np.append(vi_test, vi_inbk_test)\n",
    "        vi_vali = np.append(vi_vali, vi_inbk_vali)\n",
    "\n",
    "    #train_x, test_x, train_y, test_y = train_test_split(data_df.loc[:,XV], data_df.loc[:,['VDMSP']], test_size=0.4, random_state=42)\n",
    "\n",
    "    train_x = data_df.loc[np.int32(vi_train),X_var]\n",
    "    train_y = (data_df.loc[np.int32(vi_train),YV])\n",
    "\n",
    "\n",
    "    test_x = data_df.loc[np.int32(vi_test),X_var]\n",
    "    test_y = (data_df.loc[np.int32(vi_test),YV])\n",
    "\n",
    "    vali_x = data_df.loc[np.int32(vi_vali),X_var]\n",
    "    vali_y = (data_df.loc[np.int32(vi_vali),YV])\n",
    "\n",
    "    test_Africa_x = test_x.loc[data_df.loc[np.int32(vi_test),'Conti_most']== 'Africa',:]\n",
    "    test_Africa_y = test_y.loc[data_df.loc[np.int32(vi_test),'Conti_most']== 'Africa',:]\n",
    "    test_Africa_x = test_Africa_x.append(vali_x.loc[data_df.loc[np.int32(vi_vali),'Conti_most']== 'Africa',:])\n",
    "    test_Africa_y = test_Africa_y.append(vali_y.loc[data_df.loc[np.int32(vi_vali),'Conti_most']== 'Africa',:])\n",
    "    print('Africa sample size: ',test_Africa_x.shape)\n",
    "        \n",
    "        \n",
    "   \n",
    "        \n",
    "        \n",
    "    if True: # Is or Not Normalization : Is for better or worse?\n",
    "        scaler_x.fit_transform(data_df.loc[ :,X_var])\n",
    "        scaler_y.fit_transform(data_df.loc[ :,YV])\n",
    "        train_x = scaler_x.transform(train_x)\n",
    "        train_y = scaler_y.transform(train_y)\n",
    "        test_x = scaler_x.transform(test_x)\n",
    "        test_y = scaler_y.transform(test_y)\n",
    "        vali_x = scaler_x.transform(vali_x)\n",
    "        vali_y = scaler_y.transform(vali_y)\n",
    "        pred_x = scaler_x.transform(pred_x)\n",
    "        test_Africa_x = scaler_x.transform(test_Africa_x)\n",
    "        test_Africa_y = scaler_y.transform(test_Africa_y)\n",
    "\n",
    "\n",
    "    # define a DNN model\n",
    "    def Create_model():\n",
    "        model = tf.keras.Sequential([\n",
    "            #tf.keras.layers.BatchNormalization((train_x.shape[1],)),\n",
    "            tf.keras.layers.Dense(10, activation='relu', # After testing different activation function, relu get a better accuracy\n",
    "                                  #kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.01,l2=0.01),\n",
    "                                  #kernel_initializer='random_uniform', \n",
    "                                  input_shape=(train_x.shape[1],),use_bias=True,\n",
    "                                 ),\n",
    "            #tf.keras.layers.Dropout(0.2),\n",
    "            \n",
    "            tf.keras.layers.Dense(10, activation='relu', # After testing different activation function, relu get a better accuracy\n",
    "                                  #kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.01,l2=0.01),\n",
    "                                  #kernel_initializer='random_uniform', \n",
    "                                  input_shape=(train_x.shape[1],),use_bias=True,\n",
    "                                 ),\n",
    "            #tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(1)\n",
    "        \n",
    "        ])\n",
    "\n",
    "        # compile the model\n",
    "        Adam = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
    "        sgd = tf.keras.optimizers.SGD(learning_rate = 0.01)\n",
    "        RMSProp = tf.keras.optimizers.RMSprop(learning_rate = 0.01)\n",
    "        model.compile(optimizer=Adam, loss='mse')\n",
    "        return model\n",
    "\n",
    "    model =  Create_model()\n",
    "    # Train the model\n",
    "    # Create a callback that saves the model's weights\n",
    "    checkpoint_path = 'Light_GDP2.ckpt'\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                     save_weights_only=True,\n",
    "                                                     verbose=0)\n",
    "    model.fit(train_x, \n",
    "              train_y, \n",
    "              epochs=100, \n",
    "              batch_size=10000,\n",
    "              #validation_split=0.05,\n",
    "              validation_data=(vali_x, vali_y),\n",
    "              callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 20)],\n",
    "              verbose = 0\n",
    "             )\n",
    "    \n",
    "    model.summary()\n",
    "    #model.save(\"Light_GDP2_model_GIDLag.h5\")\n",
    "    plt.plot(model.history.history['loss'], c = 'blue')\n",
    "    plt.plot(model.history.history['val_loss'], c = 'red')\n",
    "    plt.show()\n",
    "\n",
    "    # Predition\n",
    "    pred_y = model.predict(test_x,batch_size=100000)\n",
    "\n",
    "    # plot the results\n",
    "    import matplotlib.pyplot as plt\n",
    "    # test the model\n",
    "    model.evaluate(test_x, test_y,verbose=2)\n",
    "    #print('Test loss:', test_loss)\n",
    "    #print('Test accuracy:', test_acc)\n",
    "\n",
    "    #plt.plot(train_x, train_y, '.', label='train data')\n",
    "\n",
    "    pred_train = model.predict(train_x)\n",
    "    plt.plot(train_y, pred_train, '.', label='model train')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    pred_vali = model.predict(vali_x)\n",
    "    plt.plot(vali_y, pred_vali, '.', label='model train')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    pred_test = model.predict(test_x)\n",
    "    plt.plot(test_y, pred_test, '.', label='model test')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    pred_test_Africa = model.predict(test_Africa_x)\n",
    "    plt.plot(test_Africa_y, pred_test_Africa, '.', label='test Africa')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    ## Correlation coefficient\n",
    "    import scipy.stats as stats\n",
    "    print(stats.pearsonr((np.array(train_y)).reshape(-1,), (pred_train).reshape(-1,)))\n",
    "    print(stats.pearsonr((np.array(vali_y)).reshape(-1,), (pred_vali).reshape(-1,)))\n",
    "    print(stats.pearsonr((np.array(test_y)).reshape(-1,), (pred_test).reshape(-1,)))\n",
    "    print(stats.pearsonr(test_Africa_y.reshape(-1,), pred_test_Africa.reshape(-1,)))\n",
    "    \n",
    "    metrics_mat[simi,0] = stats.pearsonr((np.array(train_y)).reshape(-1,), (pred_train).reshape(-1,))[0]\n",
    "    metrics_mat[simi,1] = stats.pearsonr((np.array(vali_y)).reshape(-1,), (pred_vali).reshape(-1,))[0]\n",
    "    metrics_mat[simi,2] = stats.pearsonr((np.array(test_y)).reshape(-1,), (pred_test).reshape(-1,))[0]\n",
    "    metrics_mat[simi,3] = stats.pearsonr(test_Africa_y.reshape(-1,), pred_test_Africa.reshape(-1,))[0]\n",
    "    \n",
    "    print(data_df.shape)\n",
    "\n",
    "    ######### prediction\n",
    "    import os\n",
    "    wksp = r'E:\\0 job\\data\\Light_GDP\\Pred_Continue\\GDP/'\n",
    "    os.chdir(wksp)\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from ipyleaflet import Map, Marker\n",
    "    import geopandas as gpd\n",
    "    import rasterio as rio\n",
    "    import matplotlib.pyplot as plt\n",
    "    from scipy.signal import convolve2d\n",
    "    import tensorflow as tf\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    #model_pred =  tf.keras.models.load_model('./Light_GDP2_model_GIDLag.h5')\n",
    "    model_pred = model\n",
    "    # Predition\n",
    "\n",
    "    pred_y = model_pred.predict(pred_x,batch_size=10000)\n",
    "    pred_y = scaler_y.inverse_transform(pred_y)\n",
    "    pred_df['pred'+str(simi)] = pred_y\n",
    "    print(simi)\n",
    "pred_df.to_csv(r'E:\\0 job\\data\\Light_GDP\\Pred_Continue\\GDP/0 MLPPredGDP_Cls15onehotNonYearlCPGDPepochs100Node10.csv',sep = ',',index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e2d50e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
